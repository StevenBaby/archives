# 信息论与编码

[annotation]: [id] (1f320295-eb64-4b4e-aced-db913bec8cc6)
[annotation]: [status] (public)
[annotation]: [create_time] (2021-09-09 13:25:38)
[annotation]: [category] (读书笔记)
[annotation]: [tags] (研究生课程|信息论|编码)
[annotation]: [comments] (true)
[annotation]: [url] (http://blog.ccyg.studio/article/1f320295-eb64-4b4e-aced-db913bec8cc6)

主讲教师：吕克伟 王丽萍

## 概述

熵 (Entropy) 的起源：热力学第二定律

- 克劳修斯表述：
    - 不可能将热从低温物体传至高温物体而不引起其它变化
    - 热量不能自发的从低温物体传向高温物体
- 开尔文-普朗克表述：
    - 不可能从单一热源吸取热量，并将这热量变为功，而不产生其他影响

- 克劳修斯：《热力推动说》

---

- 克劳德·香农 - 《通信的数学理论》
- 保密通信的通信理论
    - 保密系统模型

- fuzzy extractor

基于 Lattice 的信息传输

- SVP
- CVP
- SIVP

保密系统的密钥量越小，其中含有的关于明文的信息量就越大

密文与文明之间的互信息为零

存在完美的保密系统

-----

什么是信息

信息是使概率分布发生改变的东西

----

1928 哈特莱 《信息传输》 **信息是选择的自由度**

- 维纳：信息既不是物质，也不是能量，信息就是信息

----

39 * x * 259

- 信号
- 信息
- 消息

```
信源 --编码--> 信道 --解码--> 信宿
```

信息论是一门应用概率论

---

香农三大定理：

- 香农第一定理：可变长无失真信源编码定理
- 香农第二定理：有噪信道编码定理
- 香农第三定理：保失真度 TODO

确定概率模型

---

## 信源及其信息量

信源 $S$ 输出以符号形式出现的具体消息，消息随机的。

可以概率统计的形式来描述

信源面临的问题：

1. 定量描述问题：如何计算信源的信息量
2. 信源编码问题：如何有效的表示信源的输出或信源信息的载体形式

----

信源大致的分类：

- 时间与幅度：
    - 离散信源：信源发出的消息在时间和幅度上都是离散的
    - 连续信源：信源发出的消息在时间和幅度上都是连续的
- 依据各维随机变量的概率分布是否随着时间推移而变化：
    - 平稳信源：每维概率分布几乎相同
    - 非平稳信源
- 依据随机变量间的是否统计独立:
    - 有记忆信源
    - 无记忆信源

主要研究的对象：离散信源、平稳信源

- 无记忆的
    - 单符号
    - 扩展
- 有记忆：
    - 记忆长度有限
    - 记忆长度无线

----

### 单符号离散信源

- 每次信源只发出一个符号，代表一个消息；
- 能够输出的消息是 **有限** 或 **可数** 的

可看成是一个随机事件，可抽象成 一维随机变量

--- 

信源 $S$

数学模型 $S = \begin{pmatrix} X \\ P(X) \end{pmatrix}$ 是一维随机变量分布律 

$0 \leqslant P(x_i) \leqslant 1$

$\displaystyle \sum_{i=1}^n P(x_i) = 1$

其中：

- $X$ 表示信源输出消息的全体
- $x_i$ 表示某一特定消息
- $P(x_i)$ 表示 $x_i$ 出现的概率
- $n$ 表示 $S$ 输出消息的个数

信源每次发送一个符号 --> 单符号信源

信源发出的符号是随机的，因此使用随机变量 $X$ 来表示 $S$

----

例子：假定一根电线上串联着 8 个灯泡 $\{x_1, x_2, \cdots, x_8\}$，每个灯泡损坏的概率是 $\displaystyle P(x_i) = {1 \over 8}$，如果有且仅有一个灯泡坏了，但不知道是哪个，试用万用表测量电路，获取足够的信息来确定损坏的灯泡 $x_i$？(要求测量次数最少)

分析：每个灯泡等概率的发生，$P(x_i) = {1 \over 8}$，对应的不确定性是 $P(x_i)$ 的负对数，记为 $I(P(x_i)) = -\log_2 P(x_i)$

$$S(x, P(x)) = \begin{pmatrix}
x_1, x_2, \cdots, x_8 \\\displaystyle
{1 \over 8}, {1 \over 8}, \cdots, {1 \over 8} \\
\end{pmatrix}$$

解：类似二分查找

- 没测量时 $P(x_i) = {1 \over 8}, I(P(x_i)) = 3$
- 第一次测量 $P(x_i) = {1 \over 4}, I(P(x_i)) = 2$
- 第二次测量 $P(x_i) = {1 \over 2}, I(P(x_i)) = 1$
- 第三次测量 $P(x_i) = 1, I(P(x_i)) = 0$

也就是说，我们至少需要 3 比特的信息可确定具体的灯泡。

--- 

收到某个消息 $m$ 获得信息量记为 $I(m)$ = 不确定性减少的量，也就是收到 $m$ 之前关于 $m$ 事件发生的不确定性 - 收到 $m$ 之后关于 $m$ 事件发生的不确定性；

----

定义：一个随机事件发生某一结果后，所带来的信息量，称为该事件的 **自信息量**，简称 **自信息**；若随机事件 $x$ 发生的概率为 $P(x)$，则定义其自信息量为:

$$I(x) = - \log_2 P(x)$$

也就是 $x$ 事件发生前，不确定性的大小，$x$ 事件发生后，事件 $x$ 含有或提供的信息量。

---

例：甲乙丙有三个不透明的袋子：

1. 甲袋中放入 $n$ 个不同的欧姆值得电阻 $\Omega$，随机从中选取一个，随机拿出一个并对齐电阻值先猜测；记：其信息量为 $I(a) = f(p(a)) = f({1 \over a})$
2. 乙袋有 $m$ 个不同瓦数的电阻，随机选取一个，对其功率先进行猜测（假设等概率）； 

    自信息量为：$I(y_i) = f(p(y_i)) = f({1 \over m})$

3. 丙袋中有 $n$ 中不同阻值的电阻，每种电阻又有 $m$ 中不同的功率；

    自信息量为 $I(c_i) = f(p(c_i))  = f({1 \over mn})$

    因此时间丙 = 时间甲 和 事件乙 联合发生 

    $I(c_n) = I(x_i) + I(y_j)$

----

信息的单位：

- $\log_2 P(x)$ 单位 比特 bit
- $\ln P(x)$ 单位 奈特 nat
- $\lg P(x)$ 单位 hat
- $\log_r P(x)$ r - 进制


$$I(x) = - \log P(x)$$

---

联合自信息量

两个随机事件的离散信源，$X = (x_1,x_2,\cdots, x_n)$ $Y = (y_1, y_2, \cdots, y_m)$，其分布为

$$
S(x, y) = f(x, y, P(xy)) = 二维分布 p(x_iy_j)$$


$x_iy_i$ 同时发生， $p(x_iy_j)$ $I(x_iy_j) = - \log p(x_iy_j)$

若相互独立，则： $I(x_iy_j) = I(x_i) + I(y_j)$

等于条件自信息：$P(x_iy_j) = p(y_j)p(x_i|y_j)  = p(x_i)p(y_j|x_i)$

----

条件自信息量

有二维联合分布 $X,Y$

对于事件 $x_i, y_j$，在给定 $y_i$ 的条件下，事件 $x_i$ 的条件自信息量：

$$I(x_i|y_j) = -\log p(x_i|y_j)$$

$I(xy) = -\log P(xy) = I(x) + I(y|x) = I(y) + I(x|y)$

---

平均信息量 => 熵


TODO:

$$S(x) = \left(\begin{aligned}
&x \\ &p(x)
\end{aligned} \right) = \begin{pmatrix}
x_1, x_2, \cdots, x_8 \\\displaystyle
p(x_1), {1 \over 8}, \cdots, {1 \over 8} \\
\end{pmatrix}$$ 


该信源的平均信息量为心愿中各个离散消息的自信息量的数学期望

$$E[I(x)] = \sum_{i = 1}^n P(x_i) I(x_i)$$

称之为 **信息熵**，简称 **熵 (entropy)**；

$$H(x) = E[I(x)] = -\sum_{i = 1}^n P(x_i) \log P(x_i)$$

---

熵反应分布的均匀性，等可能分布的熵最大，确定事件熵最小；

$$H(x) = -p\log p - (1 - p) \log (1 - p)$$

---

作业：信源有 6 种符号输出，概率空间 

TODO:

x P(x)

A B C D E F

0.5 0.25 0.125 0.05 0.05 0.025

1. 计算 $H(x)$
2. 假定符号相互独立，计算

$I(ABABBA)$ 与 $I(FDDFDF)$

并将其与 6 位符号的平均信息量进行比较；

---

### 条件熵

对于联合符号集合，条件熵是 **条件自信息量** 的 **数学期望**；即 给定  $y$ 的条件下，随机变量 $x$ 的条件熵 $H(x|y)$

$$H(x|y) = \sum_{i = 1}^n\sum_{j=1}^m P(x_iy_j)I(x_i|y_j) = - \sum_{i = 1}^n\sum_{j=1}^m P(x_iy_j)\log P(x_i| y_j)$$

特别地，取 $y=y_0$ $\displaystyle H(x|y_0) = \sum_{y_0} p(y_0)H(X|y_0)$

若 $x,y$ 相互独立，则 

$$H(x|y) = \sum_{i = 1}^n\sum_{j=1}^m P(x_i)P(y_j)I(x_i) = - \sum_{i = 1}^n\sum_{j=1}^m P(x_i)P(y_j)\log P(x_i)$$

----

已知 $X,Y \in \{0, 1\}$，构成的联合概率空间

00 01 10 11
1/8 3/8 3/8 1/8

计算 H(X|Y)

$$H(X|Y)$$

---

联合熵

给定联合符号集，XY, x_iy_j

对每对元素 x_iy_j 的联合自信息量取 **期望**，记为联合熵；记：

$$H(XY) = \sum \sum P(xy)I(XY)$$
$$= - \sum \sum P(xy) \log p(XY)$$
$$= - \sum \sum P(xy) \log (p(y|x)p(x))$$
$$= - \sum \sum P(xy) [\log p(y|x) + \log p(x)]$$
$$= H(Y|X) + H(x) = H(X|Y) + H(Y)$$
$$\xlongequal{独立} H(X) + H(Y)$$

----

例子：二元系统符号 {0， 1} 发送信息时，我们希望得到正确的信息 信道可能出错，由于存在失真，传输时会产生错误，有如下事件发生：

- $U_0$ 发出 0，概率为 1/2
- $U_1$ 发出 1，概率为 1/2
- $V_0$ 收到 0 $P(v_0|u_0) = 3/4$
- $V_1$ 收到 1 $P(v_0|u_1) = 1/4$

发送 1 收到 0 的概率为 1/2

1. 已知发出一个 0，求收到符号后得到的平均信息量？

$U_0$ 发生，$P(U_0) = 1/2$

$P(V_0| U_0) = -\sum p(v_i|u_0) \log p(v_i|u_0) = 0.82$ bit/sign

---

2. 已知发出的符号，计算收到符号后的平均信息量，

计算 $H(V|U) = -\sum \sum p(u_iv_j) \log p(v_i|u_j) = 0.91 v_j|u_i$

3. 得到发出和接受的符号所得的平均信息量

$$H(UV) = 1.91$$

4. 已知 V 再被告知发出的符号

$H(U|V) = 0.95$ 

---

熵的性质：

- 非负性 $H(x) \geqslant 0$
- 对称性：和具体事件发生的先后顺序无关；

----

最大离散熵定理：

> 信源 $X$ 中包含 $n$ 个不同的离散消息；则：
> 
> $$H(x) \leqslant \log n$$
> 
> 当且仅当 $X$ 中各个消息出现的概率相等时：等号成立；

证明：

当 $x > 0$ 时， $\ln x \leqslant x - 1$

$$H(x) - \log n = -\sum p(x) \log p(x) - \sum p(x_i) \log n$$
$$= \sum p(x_i) \log {1 \over n p(x_i)}$$
$$\leqslant \sum p(x_i) ({ 1\over n P(x_i)} - 1) \log e = 0$$

---

可加性：

$$H(X_1X_2\cdots X_N) = H(X_1) + H(X_2|X_1) + \cdots + H(X_N|X_1\cdots X_{N-1})$$
$$\xlongequal{独立} H(X_1) + H(X_2) + \cdots + H(X_N) = \sum_{i=1}^N H(X_i)$$


---

香农辅助定理：

> 对于任意两个消息，个数相同的信源 $X,Y$，那么就有：

$$-\sum_{i = 1}^n p(x_i) \log p(x_i) \leqslant - \sum_{i = 1}^n p(x_i) \log p(y_i)$$

记 

$$\sum_{i = 1}^n p(x_i) \log {p(y_i) \over p(x_i)} \leqslant \log(\sum_{i=1}^n p(y_i))$$

$$
\begin{aligned}
H(X|Y) =& -\sum_{ij} p(x_iy_i) \log p(x_i |y_i) \\
=& -\sum_{ij} p(x_iy_i) \log p(x_i |y_i) \\
 \leqslant& H(x)\\
\end{aligned}$$

条件熵 <= 熵

---- 

确定性：

$$H(1, 0) = H(1, 0, \cdots, 0) = 0$$

----

扩展性：？

1. $$\lim_{e \to 0} H_{e + 1} (P_1, P_2, \cdots, P_e - e, e) = H_e(p_1, p_2, \cdots, P_e)$$

2. 若信源 $(X, P(X)) = 分布律(x_i, P_i)$

其中有一事件被划分成 $m$ 个子时间，而这 $m$ 个子事件的概率和等于原事件中的概率；

$$(x_n, P_n) = (Y_{ni}, Q_{ni}) i \in [1, m]$$

$$(\overline{X}, P_{\overline{X}}) = (Y_{ni}, Q_{ni}) i \in [1, m]$$

$$ H(\overline{X}) \geqslant H(X)$$

----

作业2：

上凸性

任意 $0 \leq \theta \leqslant 1$ $H(\theta p_1 + (1 - \theta)p_2) \geqslant \theta H(P_1) + (1 - \theta) H(p_2)$

$$H(XY) = H(X) + H(Y|X) \leqslant H(X) + H(Y)$$

---

互信息：

称一个事件 $Y$ 所给出的关于另一个事件 $X$ 的信息，称为 **互信息**，记为 $I(x;y)$

定义：log( x 的后验概率 / 先验概率)

$$I(x;y) = \log {P(x|y) \over P(x)} = I(x) - I(x|y)$$
$$\log {p(x|y) p(y) \over p(x|P(y))} = -I(xy) + I(x) + I(y)$$

$$I(x;y) = I(y;x)$$

互信息有可能小于零；

----

在联合分布 $XYZ$ 中，给定 $Z$ 的条件下 $X,Y$ 之间的互信息量定义为 如下条件互信息量；

$$
I(x;y|Z) = \log {P(x|yz) \over p(x | z)}
$$

----

整体考虑平均互信息

称互信息量 $I(x;y)$ 在 $XY$ 的联合概率空间中的统计平均值为 **平均互信息量**，记为 $I(X;Y)$

$$I(X;Y) = \sum_{i = 1}^n \sum_{j=1}^m p(x_i y_j) \log {p(x_i | y_j) \over p(x_i)}$$

称之为 $Y$ 对 $X$ 的平均互信息量；

同样我们也可定义 $X$ 对 $Y$ 的平均互信息量 $I(Y;X)$

$$I(Y;X) = \sum_{i = 1}^n \sum_{j=1}^m p(x_i y_j) \log {p(y_j | x_i) \over p(y_j)}$$

$$I(X;Y) = I(Y;X) = H(X) + H(Y) - H(XY)$$
$$I(X;Y) = I(Y;X) = H(X) + H(X|Y)$$
$$I(X;Y) = I(Y;X) = H(Y) + H(Y|X)$$


$H(X|Y)$ 称为 信道的疑义度，噪声熵；

---

习题：已知信源 (x, P(x))

x_1 x_2

1/2 1/2

在如图所示的信道上，求 $I(X;Y)$ $H(X|Y)$

$H(Y|X)$ $H(XY)$


---

$$\begin{aligned}
I(x; y) =& H(x) + H(y) - H(xy) \\
I(x; y) =& H(x) - H(x|y) \\
I(x; y) =& H(y) - H(y|x) \\
I(x; y) \leqslant& \min \{H(x), H(y)\} \\
\end{aligned}$$

$P(y|x)$ 为信道的转移概率；

$I(y;x) = f(P(x), P(y|x))$

其中

- $P(x)$ 对应信源
- $P(y|x)$ 对应信道

则：

- 若信源固定，$f$ 则为信道的函数
- 若信道固定，$f$ 则为信源的函数

---

当条件概率分布 $P(y|x)$ 给定时，$I(x;y)$ 是输入信源概率分布 $P(x)$ 严格上凸函数；

$$\begin{aligned}
\end{aligned}$$

todo 1

当固定输入分布 $P(x)$ 给定时，$I(x;y)$ 是条件概率分布 $P(y|x)$ 严格下凸函数；

两组信道： 

- $P_1(Y|X)$ , $(X, P_1(X))$
- $P_2(Y|X)$ , $(X, P_2(X))$

记 $P(Y|X) = \alpha P_1(Y|X) + (1 - \alpha) P_2(Y|X)$

todo 2


---

作业1：二元对称信道 如下图所示

todo3：信道的图

信源为

$$S(X) = \begin{pmatrix}
X \\
P(X)
\end{pmatrix} = \begin{pmatrix}
0 & 1\\
\omega & 1-\omega = \overline{\omega}
\end{pmatrix}$$

1. 计算平均互信息量 $I(X;Y)$；
2. 当信道固定时，讨论 $I(X;Y)$ 的性质，画出图像，找出极值，并解释物理意义；
3. 当信源固定时，讨论 $I(X;Y)$ 的性质，画出图像，找出极值，并解释物理意义； 

---

## 数据处理定理

> 定义：平均条件互信息量
> 
> $$I(X;Y|Z) = E[I(x;y|z)] = \sum\sum\sum P(xyz) \log {p(x|yz) \over p(x|z)}$$
> $$I(X;YZ) = E[I(x;yz)] = \sum\sum\sum P(xyz) \log {p(x|yz) \over p(x)}$$

若随机变量 $X,Y,Z$ 构成一个 **马尔科夫链**，则：

$$I(X;Z) \leqslant I(X;Y)$$

当且仅当 $P(X|YZ) = P(X|Z)$ 时，等号成立


---

作业：二元的无记忆信道如图所示

todo4

将 4 个等概率分布的消息，$m_1, m_2, m_3, m_4$ 输入信道并传输；这 4 个消息的编码为：

| m1 | m2 | m3 | m4 |
| - | - | - | - |
| 00 | 01 | 10 | 11

计算：

1. 输入 $m_i$ 和输出第一个符号为 $0$ 的互信息；
2. 输入 $m_i$ 输出的符号位 $00$ 的互信息；

---

现实中，信源输出的都是一些符号序列

$$X = X_1X_2 \cdots X_n, n \geqslant 2$$

$P(X) = \prod_{i = 1}^n P(X_i)$

离散多符号信源，扩展信源；

---

无记忆扩展信源，每次发出的是一组含有两个以上符号的序列代表一个消息；


$$X = X_1X_2 \cdots X_n, n < \infty$$

并且这些符号之间相互独立，这样的信源，我们称为无记忆扩展信源；或离散无记忆扩展信源；

这里我们称 $n$ 为扩展次数；

---

离散的无记忆二进制信源 $X$ 的二次扩展信源，有四种信号记为

$$(x^2 P(x^2)) = \begin{pmatrix}
\alpha_1 & \alpha_2 & \alpha_3 & \alpha_4 \\
P(\alpha_1) & P(\alpha_2) & P(\alpha_3) & P(\alpha_4) \\
\end{pmatrix}$$

todo

----

如果随着事件的推移，若信源的统计特性不变或一段时间内不变，这样的信源，我们称为平稳信源，统计特性很稳定；否则的话，称为非平稳信源；

---

离散平稳信源：平稳意味着符号序列的统计特性与时间的起点无关；

对于随机变量序列 $X = X_1\cdots X_n$

若任意两个不同时刻， $i,j$ 信源发出消息的概率分布完全相同，即对于 一维的平稳信源，

$$\begin{aligned}
P(X_i = X_1) =& P(X_j = x_1) \\
\vdots \\
P(X_i = X_n) =& P(X_j = x_n) \\
\end{aligned}$$

todo page 22

---

定义平均符号熵 todo page 22

---

$n$ 维平稳信源；

---

定义极限熵 page 22；

## 马尔可夫信源

- 关联性
- 平稳性

----

## 参考资料

- [A Mathematical Theory of Communication C.E.Shannon 1963](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)
- 信息论 本质 多样性 统一

