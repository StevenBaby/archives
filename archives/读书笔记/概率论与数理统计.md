# 概率论与数理统计

[annotation]: <id> (2833b864-1b83-4016-8bf7-d9c44e299b75)
[annotation]: <status> (public)
[annotation]: <create_time> (2019-05-30 11:48:30)
[annotation]: <category> (读书笔记)
[annotation]: <comments> (false)

> 原文链接：<http://blog.ccyg.studio/article/2833b864-1b83-4016-8bf7-d9c44e299b75>

---

## 事件的概率 <sub><small>2019-05-30</small></sub>

主观概率可以理解为种心态或倾向性。究其根由大抵有二：一是根据其经验和知识。例如多余明天会可能下雨来说，若某人在该城市住了30年，又是个有些气象知识的人，他在作出可能性大小的估计时，多半会使用这些经验和知识，这将会使他的估计较易为人所相信。从这一点说，所谓主观概率也可有其客观背景，终究不同于信口雌黄。二是根据其利害关系，拿上例来说，若对某人而言下雨并不会造成多大问题而带伞又增加不少麻烦，则其心态将倾向于去把A的可能性高估一些。

主观概率的特点是：它不是在竖实的客观理由尪础上为人们所公认的，因而看来应被科学所否定（科学是以探讨客观真理为任务的）本书作者说不清楚这问题该如何全面地去理解，但不同意简单的全盘否定的态度。理由有三：

1. 这个概念有广泛的生活基础。我们几乎尤时不在估计种种情况出现的可能性如何，而不匝的 人很少能在“客观”的基础上达成一致。
2. 这可能反映认识主体的 一种倾向性，而有其社会意义例如，“若问三年后经济形势会得到 根本改善”的可能性大小怎样，则不同经济状况、社会地位以至政治倾向的人，会作出有差异的估计。就个别估计而言可能谈不上多大道理，但从总体而言，则反映了社会上广大群众对长远发展的信心如何．对社会学家乃至决策者来说，这是很有用的资料。
3. 在涉及利益（经济和其他的）得失的决策问题中，处于不同地位和掌握情报多少不同的人，对某事件可能性大小要参照这些情况及可能的后果去作衡量。适合于某人的决策，虽则风险较小，不必适合于另一个人，因对他而言，这一决策可能风险仍太大．因此，主观概率这个概念也有其使用基础。事实上，许多决策都难免要包含个人判断的成分，而这就是主管概率。

### 古典概型

设一个试验有N个等可能的结果，而事件E恰包含其中的M个结果，则事件E的概率，记为P(E),定义为

$$P(E) = M/N$$

### 几何概型

古典概率的局限性很显然：它只能用于全部试验结果为有限个，且等可能性成立的情况．但在某些情况下，这概念可稍稍引申到试验结果有尤限多的情况，这就是所谓“几何概率＂。

### 概率统计的定义

从实用的角度看，概率的统计定义无非是一种通过实验去估计事件概率的方法。拿＂掷骰子”这个例子来说，若骰子并非质地均匀的正方体，则投掷时各面出现的概率不必相同。这时，"出现幺点" 这个个事件 $E_1$ 的概率有多大，已无法仅通过一种理论的考虑来确定．但我们可以做实验：反复地将这骰子投掷大量的次数，例如n次．若在这n次投掷中么共出现$m_1$次，则称$m_1$是$E_1$这个事件在这n次试验（每次投掷算作一个试验）中的＂频率＂．概率的统计定义的要旨是说，就拿这个频率$\frac{m_1}{n}$作为事件$E_1$的概率$P(E_1)$的估计．这个概念的直观背景很简单：一事件出现的可能性大小，应由在多次重复试验中其出现的频繁程度去刻画。

**概率的统计定义** 的重要性，不在于它提供了一种定义概率的方法 它实际上没有提供这种方法，因为你永远不可能依据这个定义确切地定出任何一个事件的概率。其重要性在于两点：

- 一是提供了一种估计概率的方法
- 二是它提供了一种检验理论正确与否的准则

### 概率的公式化定义

成功地将概率论实现公理化的，是现代前苏联大数学家柯尔莫哥洛夫，时间在1933年．值得赞赏的不止在于他实现了概率论的公理化，还在于他提出的公理为数很少且极为简单，而在这么一个基础上建立起了概率论的宏伟大厦。

### 条件概率

一般讲，条件概率就是在附加一定的条件之下所计算的概率．从广义的意义上说，任何概率都是条件概率，因为，我们是在一定的试验之下去考虑事件的概率的，而试验即规定有条件．在概率论中，规定试验的那些基础条件被看作是已定不变的．如果不再加入其他条件或假定，则算出的概率就叫做“无条件概率＂，就是通常所说的概率．当说到“条件概率”时，总是指另外附加的条件，其形式可归恃为"已知某事件发生了"．

## 随机变量及其分布

## 随机变量的数字特征 <sub><small>2019-05-30</small></sub>

### 期望

**期望**这个名词源出赌博，听起来不大通俗化或形象易懂，本不是一个很恰当的命名，但它在概率论中已源远流长获得大家公认，也就站住了脚根。另一个名词 **均值** 形象易懂，也很常用，将在下文解释。

期望的定义：设随机变量X 只取有限个可能值$a_1,\cdots,a_m$。其概率分布为$P(X=i)=p_i, i=1,\cdots,m$。则$X$的数学期望，记为$E(X)$或$EX$, 定义为：

$$
E(X) = a_1p1 + a_2p_2 + \cdots + a_mp_m
$$

数学期望也常称为**均值**，即“随机变量取值的平均值”之意，当然这个平均，是指以概率为权的加权平均。

> 实际上期望是全体样本的均值，实际中，大多对全体样本进行抽样，这样均值就成了近似的期望。

当样本很大时，把数学期望 $E(X)$ 定义为级数之和：

$$
E(x) = \sum_{i=1}^{\infty}a_ip_i
$$

但当然，必须级数收敛才行，实际上我们要求更多，要求这个级数绝对收敛：

$$
E(x) = \sum_{i=1}^{\infty}|a_i|p_i < \infty
$$

如果是连续性随机变量，则如果概率密度函数为 $f(x)$，那么相应的期望就是：

$$
E(X) = \int_{-\infty}^{+\infty}xf(x)dx 
$$

---

**泊松分布的期望**

由于泊松分布服从：

$$
P\{X=k\} = \frac{\lambda^k}{k!} e ^{-\lambda}
$$

所以，泊松分布的期望为：

$$
\begin{aligned}
E(x) 
&= \sum_{k=0}^\infty k \frac{\lambda^k}{k!}e^{-\lambda}\\
&= \lambda e^{-\lambda} \sum_{k=1}^\infty \frac{\lambda ^ {k-1}}{(k - 1)!} \\
&= \lambda e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} \\
&= \lambda e^{-\lambda} e^\lambda = \lambda 
\end{aligned}
$$

$\lambda$ 就是在所指定的时间段中发生事故的平均次数

--- 
**二项分布的期望**

**均匀分布的期望**

**指数分布的期望**

**正态分布的期望**

---

### 中位数

**定义** 设连续型随机变量$X$的分布函数为$F(X)$, 则满
足条件

$$
P(X \leq m) = F(m) = \frac{1}{2}
$$

的数$m$称为 $X$ 或分布 $F$ 的中位数

在实用上，中位数用得很多，特别有不少社会统计资料，常拿中位数来刻画某种量的代表性数值，有时它比数学期望更说明问题。

例如，某社区内人的收入的中位数告诉我们：有一半的人收入低于此值，另一半高于此值。我们直观上感觉到这个值对该社区的收入情况，的确很具代表性。

它和期望值相比它的一个优点是：它受个别特大或特小值的影响很小，而期望则不然。举例而言，若该社区中有一人收人在百万元以上，则该社区的均值可能很高，而绝大多数人并不富裕，这个均值并不很有代表性。中位数则不然：它不受少量这种特大值的影响。

从理论上说，中位数与均值相比还有一个优点，即它总存在，而均值则不是对任何随机变量都存在。

虽然中位数有这些优点，但在概率统计中，无论在理论和应用上，数学期望的重要性都超过中位数，其原因有以下两方面：

- 期望有很多优良的性质，可以方便的进行数学运算
- 二是中位数本身所固有的某些缺点
    - 中位数可能不唯一
    - 对于离散型问题，有时候中位数并不是理想的中位数

### 方差

**定义** 设 $X$ 为随机变量，分布为$F$, 则：

$$
D(X) = E(X - EX)^2
$$

称为$X$(或分布$F$) 的方差，其平方根$\sqrt{D(X)}$称为$X$(或分布$F$) 的标准差

根据完全平方公式展开：

$$
\begin{aligned}
D(x)
&= E(X - EX)^2 \\
&= E(X^2 - 2XEX + (EX)^2) \\
&= E(X^2) - 2EXEX + (EX)^2 \\
&= E(X^2) - (EX)^2
\end{aligned}
$$

方差的这个形式在计算上往往较为方便

### 矩

**定义** 设$X$为随机变量，$c$ 为常数， $k$ 为正整数．则：

$$
E[(X-c)^k] 
$$

称为$X$关于$c$点的$K$阶矩

一阶原点矩就是期望．一阶中心矩 $μ_1=0$, 二阶中心矩$μ_2$ 就是$X$的方差$D(X)$. 在统计学上，高于4阶的矩极少使用。三、四阶矩有些应用，但也不很多。

### 协方差

**定义** 称 $E[(X - E(X)) (Y - E(Y))]$ 为 $X,Y$的协方差，并记为 $COV(X,Y)$

### 相关系数

**定义** 称 $\rho_{XY} = \frac{COV(X,Y)}{\sqrt{DX}\sqrt{DY}}$ 为随机变量 $X,Y$ 的相关系数。

---

## 参数估计

## 假设检验

## 回归、相关与方差分析 <sub><small>2019-05-30</small></sub>

**回归**一词的来由将在后面加以解释。在现实世界中存在着大量这样的情况：两个或多个变措之间有一些联系，但没有确切到可以严格决定的程度。

例如，人的身高X和体重Y有联系，一般表现为X大时，Y也倾向于大，但由X并不能严格地决定Y。一种农作物的面产量Y与其播种量$X_1$，施肥量 $X_2$有联系，但$X_1$ ,$X_2$ 不能严格决定Y。工业产品的质址指标Y与工艺参数和配方等有联系，但后者也不能严格决定Y。

在以上诸例及类似的例子中，Y通常称为**因变量**或**预报量**，$X,X_1,X_2$等则称为**自变量**或**预报因子**。因变量、自变量的称呼借用自函数关系，它不十分妥贴，因为，有时变量间并尤明显的因果关系存在。

---

现设在一个问题中有因变量$Y$，及自变量$X_1,X_2,\cdots,X_p$.可以设想$Y$的值由两部分构成：一部分由$X_1,X_2,\cdots,X_p$的影响所致，这一部分表为$X_1,X_2,\cdots,X_p$的函数形式$f(X_1,X_2,\cdots,X_p)$. 另一部分则由其他众多未加考虑的因素，包括随机因素的影响所致，它可视为一种随机误差，记为$e$，于是得到模型：

$$
Y = f(X_1,X_2,\cdots,X_p) + e
$$

e作为随机误差，我们要求其均值为0:

$$
E(e) = 0 
$$

于是得到：$f(X_1,X_2,\cdots,X_p)$就是在给定了自变量$X_1,X_2,\cdots,X_p$之值的条件下，因变量$Y$的条件期望值。可写为：

$$
f(X_1,X_2,\cdots,X_p) = E(Y|X_1,X_2,\cdots,X_p)
$$

函数$f(X_1,X_2,\cdots,X_p)$称为$Y$对$X_1,X_2,\cdots,X_p$的“**回归函数**”，而方程

$$
y = f(X_1,X_2,\cdots,X_p)
$$

则称为$Y$对$X_1,X_2,\cdots,X_p$的“**回归方程**”．有时在回归函数和回归方程之前加上“理论”二字，以表明它是直接来自模型，也可以说是模型的一个组成部分，而非由数据估计所得。后者称为 **经验回归函数** 和 **经验回归方程**。

设$\xi$为一随机变量，则$E(\xi-c)^2$作为$c$的函数，在$c=E(\xi)$ 处达到最小。由这个性质，可以对理论回归函数$f(X_1,X_2,\cdots,X_p)$作下面的斛释：如果我们只掌握了因素$X_1,X_2,\cdots,X_p$，而希望利用它们的值以尽可能好地逼近$Y$的值，则在均方误差最小的意义下，以使用理论回归函数为最好。

## 参考资料

- [概率论与数理统计 - 陈希孺](https://book.douban.com/subject/2201479/)