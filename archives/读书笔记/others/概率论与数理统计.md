# 概率论与数理统计

[annotation]: <id> (2833b864-1b83-4016-8bf7-d9c44e299b75)
[annotation]: <status> (public)
[annotation]: <create_time> (2019-05-30 11:48:30)
[annotation]: <category> (读书笔记)
[annotation]: <tags> (概率论|统计学)
[annotation]: <comments> (false)


## 事件的概率 <sub><small>2019-05-30</small></sub>

主观概率可以理解为种心态或倾向性。究其根由大抵有二：一是根据其经验和知识。例如多余明天会可能下雨来说，若某人在该城市住了30年，又是个有些气象知识的人，他在作出可能性大小的估计时，多半会使用这些经验和知识，这将会使他的估计较易为人所相信。从这一点说，所谓主观概率也可有其客观背景，终究不同于信口雌黄。二是根据其利害关系，拿上例来说，若对某人而言下雨并不会造成多大问题而带伞又增加不少麻烦，则其心态将倾向于去把A的可能性高估一些。

主观概率的特点是：它不是在竖实的客观理由尪础上为人们所公认的，因而看来应被科学所否定（科学是以探讨客观真理为任务的）本书作者说不清楚这问题该如何全面地去理解，但不同意简单的全盘否定的态度。理由有三：

1. 这个概念有广泛的生活基础。我们几乎尤时不在估计种种情况出现的可能性如何，而不匝的 人很少能在“客观”的基础上达成一致。
2. 这可能反映认识主体的 一种倾向性，而有其社会意义例如，“若问三年后经济形势会得到 根本改善”的可能性大小怎样，则不同经济状况、社会地位以至政治倾向的人，会作出有差异的估计。就个别估计而言可能谈不上多大道理，但从总体而言，则反映了社会上广大群众对长远发展的信心如何．对社会学家乃至决策者来说，这是很有用的资料。
3. 在涉及利益（经济和其他的）得失的决策问题中，处于不同地位和掌握情报多少不同的人，对某事件可能性大小要参照这些情况及可能的后果去作衡量。适合于某人的决策，虽则风险较小，不必适合于另一个人，因对他而言，这一决策可能风险仍太大．因此，主观概率这个概念也有其使用基础。事实上，许多决策都难免要包含个人判断的成分，而这就是主管概率。

### 古典概型

设一个试验有N个等可能的结果，而事件E恰包含其中的M个结果，则事件E的概率，记为P(E),定义为

$$P(E) = M/N$$

### 几何概型

古典概率的局限性很显然：它只能用于全部试验结果为有限个，且等可能性成立的情况．但在某些情况下，这概念可稍稍引申到试验结果有尤限多的情况，这就是所谓“几何概率＂。

### 概率统计的定义

从实用的角度看，概率的统计定义无非是一种通过实验去估计事件概率的方法。拿＂掷骰子”这个例子来说，若骰子并非质地均匀的正方体，则投掷时各面出现的概率不必相同。这时，"出现幺点" 这个个事件 $E_1$ 的概率有多大，已无法仅通过一种理论的考虑来确定．但我们可以做实验：反复地将这骰子投掷大量的次数，例如n次．若在这n次投掷中么共出现$m_1$次，则称$m_1$是$E_1$这个事件在这n次试验（每次投掷算作一个试验）中的＂频率＂．概率的统计定义的要旨是说，就拿这个频率$\frac{m_1}{n}$作为事件$E_1$的概率$P(E_1)$的估计．这个概念的直观背景很简单：一事件出现的可能性大小，应由在多次重复试验中其出现的频繁程度去刻画。

**概率的统计定义** 的重要性，不在于它提供了一种定义概率的方法 它实际上没有提供这种方法，因为你永远不可能依据这个定义确切地定出任何一个事件的概率。其重要性在于两点：

- 一是提供了一种估计概率的方法
- 二是它提供了一种检验理论正确与否的准则

### 概率的公式化定义

成功地将概率论实现公理化的，是现代前苏联大数学家柯尔莫哥洛夫，时间在1933年．值得赞赏的不止在于他实现了概率论的公理化，还在于他提出的公理为数很少且极为简单，而在这么一个基础上建立起了概率论的宏伟大厦。

### 条件概率

一般讲，条件概率就是在附加一定的条件之下所计算的概率．从广义的意义上说，任何概率都是条件概率，因为，我们是在一定的试验之下去考虑事件的概率的，而试验即规定有条件．在概率论中，规定试验的那些基础条件被看作是已定不变的．如果不再加入其他条件或假定，则算出的概率就叫做“无条件概率＂，就是通常所说的概率．当说到“条件概率”时，总是指另外附加的条件，其形式可归恃为"已知某事件发生了"．

$$
P(A|B) = \frac{P(AB)}{P(B)}
$$

### 乘法公式

$$
P(AB) = P(A|B)P(B) = P(B|A)P(A)
$$

### 全概率公式

$$
P(A) = \sum_{i = 1}^{\infty} P(B_i) P(A|B_i)
$$

### 贝叶斯公式

$$
P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{j=1}^{n} P(B_j)P(A|B_j)}
$$

---

## 随机变量及其分布

### 离散型随机变量

**定义** 设 $X$ 为离散型随机变量，其全部可能值为 $\{ a_1, a_2, \cdots \}$ 则：

$$
p_i = P(X=a_i), i=1,2,\cdots
$$

称为 $X$ 的概率函数。

显然有：

$$
p_i \geq 0, p_1 + p_2 + \cdots = 1
$$

### 0-1 分布（伯努利分布）

伯努利分布（又名两点分布或者0-1分布，是一个离散型概率分布，为纪念瑞士科学家**雅各布·伯努利**而命名) 若伯努利试验成功，则伯努利随机变量取值为1。若伯努利试验失败，则伯努利随机变量取值为0。记其成功概率为 $p(0{\leq }p{\leq }1)$ ，失败概率为 $q=1-p$。

$$
P(X=k) = p^k(1-p)^{1-k} \ (k=0,k=1)
$$

$$
EX = \sum_{i=0}^{1} P(X=k)k_i = 0 + p = p
$$

$$
DX = p(1-p)
$$

0-1 分布本质上表示了成功或者失败，正面或者反面这种相互对立的事件。

---

### 二项分布 （n 重伯努利分布）

将 0-1 分布进行多次，就得到了二项分布

$$
p(X=k) = C_n^k p^k (1-p)^{n-k}
$$

记为 $B(n,p)$

$$
EX = np
$$

$$
DX = np(1-p)
$$

二项分布是最重要的离散型概率分布之一，变量$X$服从这个分布有两个重要条件：
- 一是各次试验的条件是稳定的，这保证了事件 $A$ 的概率 $p$ 在各次试验中保持不变；
- 二是各次试验的独立性。现实生活中有许多现象程度不同地符合这些条件，
而不一定分厘不差。

---

### 泊松分布

将0-1分布 进行无穷多次，就得到了泊松分布

$$
P(X=k) = \frac{\lambda^k}{k!} e ^{-\lambda}
$$

这个分布也是最重要的离散型分布之一，它多是出现在当$X$表示在一定的时间或空间内出现的事件个数这种场合。

**泊松分布就是描述某段时间内，事件具体的发生概率**

---

### 连续性随机变量

**定义** 设连续性随机变量 $X$ 有概率分布函数 $F(x)#, 则 $F(x)$ 的导数 $f(x) = F'(x)$ , $f(x)$ 称为 $X$ 的概率密度函数。

连续荆随机变量 $X$ 的密度函数 $f(x)$ 都具有以下三条基本性
质：

- $f(x) \geq 0$
- $\int_{-\infty}^{\infty} f(x) dx = 1$
- 对于任何常数 $a < b$ 有：
    $$
    P(a \geq X \geq b) = F(b) - F(a) = \int_a^b f(x) dx
    $$

---

### 均匀分布

$$
f(x) = \left\{
\begin{aligned}
& \frac{1}{b - a}, a \leq x < b \\
& 0, 其他
\end{aligned}
\right.
$$

---

### 正态分布

$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma} e ^{\frac{-(x-\mu)^2}{2\sigma^2}}, -\infty < x < \infty
$$

记为 $X\sim N(\mu, \sigma^2)$


---

### 指数分布

$$
f(x) = \left\{
\begin{aligned}
& \lambda e^{-\lambda x}, x \geq 0 \\
& 0, x < 0 
\end{aligned}
\right.
$$

$$
EX = \frac{1}{\lambda}
$$


**指数分布是事件的时间间隔的概率**

---

## 随机变量的数字特征 <sub><small>2019-05-30</small></sub>

### 期望

**期望**这个名词源出赌博，听起来不大通俗化或形象易懂，本不是一个很恰当的命名，但它在概率论中已源远流长获得大家公认，也就站住了脚根。另一个名词 **均值** 形象易懂，也很常用，将在下文解释。

期望的定义：设随机变量X 只取有限个可能值$a_1,\cdots,a_m$。其概率分布为$P(X=i)=p_i, i=1,\cdots,m$。则$X$的数学期望，记为$E(X)$或$EX$, 定义为：

$$
E(X) = a_1p1 + a_2p_2 + \cdots + a_mp_m
$$

数学期望也常称为**均值**，即“随机变量取值的平均值”之意，当然这个平均，是指以概率为权的加权平均。

> 实际上期望是全体样本的均值，实际中，大多对全体样本进行抽样，这样均值就成了近似的期望。

当样本很大时，把数学期望 $E(X)$ 定义为级数之和：

$$
E(x) = \sum_{i=1}^{\infty}a_ip_i
$$

但当然，必须级数收敛才行，实际上我们要求更多，要求这个级数绝对收敛：

$$
E(x) = \sum_{i=1}^{\infty}|a_i|p_i < \infty
$$

如果是连续性随机变量，则如果概率密度函数为 $f(x)$，那么相应的期望就是：

$$
E(X) = \int_{-\infty}^{+\infty}xf(x)dx 
$$

---

**泊松分布的期望**

由于泊松分布服从：

$$
P\{X=k\} = \frac{\lambda^k}{k!} e ^{-\lambda}
$$

所以，泊松分布的期望为：

$$
\begin{aligned}
E(x) 
&= \sum_{k=0}^\infty k \frac{\lambda^k}{k!}e^{-\lambda}\\
&= \lambda e^{-\lambda} \sum_{k=1}^\infty \frac{\lambda ^ {k-1}}{(k - 1)!} \\
&= \lambda e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} \\
&= \lambda e^{-\lambda} e^\lambda = \lambda 
\end{aligned}
$$

$\lambda$ 就是在所指定的时间段中发生事故的平均次数

--- 
**二项分布的期望**

**均匀分布的期望**

**指数分布的期望**

**正态分布的期望**

---

### 中位数

**定义** 设连续型随机变量$X$的分布函数为$F(X)$, 则满
足条件

$$
P(X \leq m) = F(m) = \frac{1}{2}
$$

的数$m$称为 $X$ 或分布 $F$ 的中位数

在实用上，中位数用得很多，特别有不少社会统计资料，常拿中位数来刻画某种量的代表性数值，有时它比数学期望更说明问题。

例如，某社区内人的收入的中位数告诉我们：有一半的人收入低于此值，另一半高于此值。我们直观上感觉到这个值对该社区的收入情况，的确很具代表性。

它和期望值相比它的一个优点是：它受个别特大或特小值的影响很小，而期望则不然。举例而言，若该社区中有一人收人在百万元以上，则该社区的均值可能很高，而绝大多数人并不富裕，这个均值并不很有代表性。中位数则不然：它不受少量这种特大值的影响。

从理论上说，中位数与均值相比还有一个优点，即它总存在，而均值则不是对任何随机变量都存在。

虽然中位数有这些优点，但在概率统计中，无论在理论和应用上，数学期望的重要性都超过中位数，其原因有以下两方面：

- 期望有很多优良的性质，可以方便的进行数学运算
- 二是中位数本身所固有的某些缺点
    - 中位数可能不唯一
    - 对于离散型问题，有时候中位数并不是理想的中位数

### 方差

**定义** 设 $X$ 为随机变量，分布为$F$, 则：

$$
D(X) = E(X - EX)^2
$$

称为$X$(或分布$F$) 的方差，其平方根$\sqrt{D(X)}$称为$X$(或分布$F$) 的标准差

根据完全平方公式展开：

$$
\begin{aligned}
D(x)
&= E(X - EX)^2 \\
&= E(X^2 - 2XEX + (EX)^2) \\
&= E(X^2) - 2EXEX + (EX)^2 \\
&= E(X^2) - (EX)^2
\end{aligned}
$$

方差的这个形式在计算上往往较为方便

### 矩

**定义** 设$X$为随机变量，$c$ 为常数， $k$ 为正整数．则：

$$
E[(X-c)^k] 
$$

称为$X$关于$c$点的$K$阶矩

一阶原点矩就是期望．一阶中心矩 $μ_1=0$, 二阶中心矩$μ_2$ 就是$X$的方差$D(X)$. 在统计学上，高于4阶的矩极少使用。三、四阶矩有些应用，但也不很多。

### 协方差

**定义** 称 $E[(X - E(X)) (Y - E(Y))]$ 为 $X,Y$的协方差，并记为 $COV(X,Y)$

**协** 即 **协同** 的意思，$X$的方差是 $(X-EX)$ 与 $(X-EX)$ 的乘积的期望，如今把一个 $X-EX$ 换为 $Y-EY$，其形式接近方差，又有 $X,Y$ 二者的参与，由此得出协方差的名称。

### 相关系数

**定义** 称 $\rho_{XY} = \frac{COV(X,Y)}{\sqrt{DX}\sqrt{DY}}$ 为随机变量 $X,Y$ 的相关系数。

形式上可以把相关系数视为 **标准尺度下的协方差**。协方差作为$(X-EX)(Y-EY)$的均值，依赖于$X,Y$的度量单位，选择适当单位使$X,Y$的方差都为1，则协方差就是相关系数。这样就能更好地反映$X,Y$之间的关系，不受所用单位虳影响。

由千相关系数只能刻画线性关系的程度，而不能刻画一般的函数相依关系的程度，在概率论中还引进了另一些相关性指标，以补救这个缺点．但是，这些指标都未能在应用中推开。究其原因，除了这些指标在性质上比较复杂外，还有一个重要原因：在统计学应用上，最重要的二维分布是二维正态分布，而对二维正态分布而言，相关系数是 $X,Y$ 的相关性的一个完美的刻画，没有上面指出的缺点其根据有两条：

1. 若 $(X, Y)$ 为二维正态，则即使允许你用任何函数 $M(X)$ 去逼近Y，仍以 $E[(Y-M(X))^2]$ 最小为准则，那你所得到的最佳逼近，仍是由式 $L(X) = m_2 - \sigma_1^{-1}\sigma_2\rho m_1 + \sigma_1^{-1}\sigma_2\rho X$ 决定的。故在这个场合，只须考虑线性逼近已足，而这种逼近的程度完全由相关系数决定。
2. 当 $(X, Y)$ 为二维正态时，由 $Corr(X, Y) = 0$ 能推出 $X,Y$ 独立。即在这一特定场合，独立与不相关是一回事．我们前已指出，这在一般情况并不成立。

### 大数定律

在数学中大家都注意到过这样的现象：有的时候一个有限的和很难求，但一经取极限由有限过渡到无限，则问题反而好办。例如，若要对某一有限范围的 $x$ 计算和：

$$
a_n(x) = 1 + x + \frac{x^2}{2!} + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^n}{n!}
$$

则在 $n$ 固定但很大时，很难求。而一经取极限，则有简单的结果：

$$\lim_{n \to \infty } a_n(x) = e^x$$

利用这个结果，当 $n$ 很大时，可以把 $e^x$ 作为 $a_n(x)$
的近似值。


**定义** ：设$X_1,X_2,\cdots,X_n,\cdots$ 是独立同分布的随机变量，记它们的公共均值为 $a$。又设它们的方差存在并记为 $\sigma^2$。则对任意给定的 $e > 0$ 有

$$
\lim_{n \to \infty} P(| \overline{X}_n - a | \geq \epsilon) = 0
$$

我们相信，如果抽样真是随机的（每一学生有同等被抽出的机会），则随着抽样次数增多，这样的可能性会愈来愈小。这就是上式的意思。像上式这样的收敛性，在概率论中叫做 **$\overline{X}_n$ 依概率收敛于a**。

---

**马尔科夫不等式** 若$Y$为只取非负值的随机变量，则对任给
常数 $\epsilon$ 有：

$$
P(Y \geq \epsilon) \leq \frac{E(Y)}{\epsilon}
$$

**切比雪夫不等式**

$$
P(|Y - EY| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}
$$

在概率论中，大数定理常称为 **大数定律**。这个字面上的不同，也不见得有很特殊的含义但是，**定理** 一词往往用于指那种能用数学工具严格证明的东西，而 **定律** 则不一定是这样。如牛顿的力学三大定律，电学中的欧姆定律之类。这牵涉到一个从哪个角度去看的问题。大数定律有确切数学表述，并能在一定的理论框架内证明的结果，称之为 **定理** 无疑是恰当的。可是，当我们泛泛地谈论 **平均值的稳定性**（即稳定到理论上的期望值）时，这表述了一种全人类多年的集体经验，有些哲理的味道。且这种意识也远早千现代概率论给之以严格表述之前，因此，称之为 **定律** 也不算不恰当。

---

### 中心极限定理

**定理** 设$X_1 ,X_2,\cdots,X_n,\cdots$ 为独立同分布的随机变量，$E(X_i) = a , Var(X_i) = a2 , 0 < a2 < \infty$ 则对任何实数$x$, 有：

$$
\lim_{n \to \infty} P(\frac{1}{\sqrt{n}\sigma}(X_1 + \cdots + x_n - na) \leq x) = \Phi(x)
$$

这里 $\Phi(x)$ 是标准正态分布 $N(0,1)$ 的分布函数，即：

$$
\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-t^2/2} dt
$$

注意 $X_1 + \cdots + X_n$ 有均值 $na$, 方差$n\sigma^2$。故：

$$
(X_1 + ... + X_n - na) / (\sqrt{n}\sigma)
$$

就是 $X_1 + \cdots + X_n$ 的标准化，即使其均值变为0方差变为1 , 以与
$N(0,1)$的均值方差符合。

---

## 参数估计

数理统计学就是这样一门学科：它使用概率论和数学的方法，研究怎样收集（通过试验或观察）带有随机误差的数据，并在设定的模型（称为统计模型）之下，对这种数据进行分析（称为统计分析），以对所研究的问题作出推断（称为统计推断）

### 样本均值

$$
\overline{X} = \frac{X_1 + \cdots + X_n}{n}
$$

### 样本方差

$$
S^2 = \sum_{i=1}^{n} \frac{(X_i - \overline{X})^2}{n - 1}
$$

假如你想知道一所大学里学生的平均身高是多少，一个大学好几万人，全部统计有点不现实，但是你可以先随机挑选100个人，统计他们的身高，然后计算出他们的平均值，记为 $\overline{X_1}$ 。如果你只是把 $\overline{X_1}$ 作为整体的身高平均值，误差肯定很大，因为你再随机挑选出100个人，身高平均值很可能就跟刚才计算的不同，为了使得统计结果更加精确，你需要多抽取几次，然后分别计算出他们的平均值，分别记为：$\overline{X_2},\overline{X_3},\cdots,\overline{X_n}$ 然后在把这些平均值，再做平均，记为： $E(\overline{X})$ ，这样的结果肯定比只计算一次更加精确，随着重复抽取的次数增多，这个期望值会越来越接近总体均值 $\mu$，如果满足，这就是一个无偏估计，其中统计的样本均值也是一个随机变量， $\overline{X_i}$ 就是 $\overline{X}$ 的一个取值。无偏估计的意义是：**在多次重复下，它们的平均数接近所估计的参数真值。**

介绍无偏估计的意义就是，我们计算的样本方差，希望它是总体方差的一个无偏估计，那么假如我们的样本方差是如下形式：

$$
S^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2
$$

那么，我们根据无偏估计的定义可得：

$$
\begin{aligned}
&E(S^2) \\
& = E(\frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})) \\
& = E(\frac{1}{n} \sum_{i=1}^{n} [(X_i - \mu) - (\overline{X} - \mu)]^2 \\
& = E(\frac{1}{n} \sum_{n=1}^{n}[(X_i - \mu)^2 - 2(X_i - \mu)(\overline{X} - \mu) + (\overline{X} - \mu)^2]) \\
& = E[\frac{1}{n} \sum_{n=1}^{n} (X_i - \mu)^2 - \frac{1}{n} \sum_{n=1}^{n} 2(X_i - \mu)(\overline{X} - \mu) + \frac{1}{n} \sum_{n=1}^{n} (\overline{X} - \mu)^2] \\
& = E(\frac{1}{n} \sum_{n=1}^{n}[(X_i - \mu)^2 - 2(\overline{X} - \mu)(\overline{X} - \mu) + (\overline{X} - \mu)^2]) \\
& = E(\frac{1}{n} \sum_{n=1}^{n}[(X_i - \mu)^2 - (\overline{X} - \mu)^2]) \\
& = E(\frac{1}{n} \sum_{n=1}^{n}(X_i - \mu)^2) - E[(\overline{X} - \mu)^2] < \sigma^2
\end{aligned}
$$

由上式可以看出如果除以 $n$ ，那么样本方差比总体方差的值偏小，那么该怎么修正，使得样本方差式总体方差的无偏估计呢？我们接着上式继续化简：

$$
\begin{aligned}
& E(\frac{1}{n} \sum_{n=1}^{n}(X_i - \mu)^2) - E[(\overline{X} - \mu)^2] \\
& = D(X) - D(\overline{X}) \\
& = \sigma^2 - \frac{1}{n}\sigma^2 \\
& = \frac{n - 1}{n}\sigma^2 = E(S^2)
\end{aligned}
$$

到这里得到如下式子，看到了什么？该怎修正似乎有点眉目。

$$
E(S^2) = \frac{n - 1}{n}\sigma^2
$$

所以：

$$
S^2 = \frac{n}{n-1}[\frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2] = \frac{1}{n - 1} \sum_{i=1}^{n} (X_i - \overline{X})^2
$$

### 矩估计

用样本的 $k$ 阶矩作为总体的 $k$ 阶矩的估计量，建立含待估计参数的方程。

$$
E(X^k) = f_k(\theta) \Rightarrow \frac{X_1^k + X_2^k + \cdots + X_n^k}{n} = f_k(\hat{\theta})
$$

矩估计不唯一,为了计算简单，尽可能用低阶矩。

### 极大似然估计

设一个盒子里装有一定量的白球和黑球，试估计其中黑球比例 $p$ 。 假定进行 10 次有放回的抽取，抽到 3 个黑球。黑球个数 $X \sim b (p,10)$

发生这一结果的概率 $P(X=3)=C_{10}^3p^3(1-p)^7$

p=0.1时，P = 0.0574； p = 0.4 时， P = 0.215； p = 0.3时， P = 0.2668 。

显然，取不同的 $p$ 得到的 $P$ 有大有小，在我们已得到样本数据后，$P$ 为最大值时是最符合样本数据的。

**极大似然估计方法的基本思想是以最大概率解释样本数据**。

极大似然估计的实现过程

1. 设总体分布的概率函数为 $f ( x;\theta)$ ，其中 $\theta$ 是一组未知参数， $\Theta$ 称为 参数空间，即参数 $\theta$ 可能取值的集合。
2. $x_1,x_2,...,x_n$ 是来自该总体的样本观测值，则样本值发生的联合概率函数是关 于 $\theta$ 的函数，用 $L(\theta , x_1,x_2,...,x_n)$ 表示，简记为 $L(\theta)$：
    $$
    L(\theta)=L(\theta , x_1,x_2,...,x_n)=\prod_{k=1}^{n}f(x_k,\theta)
    $$
    称为样本值的似然函数
3. 函数 $\theta(x_1,x_2,...,x_n)$ 满足 $L(\theta)=max L(\theta)$ ，则称统计量 $\theta(X_1,X_2,...,X_n)$ 为参 数 $\theta$ 的极大似然估计量。

## 假设检验

## 回归、相关与方差分析 <sub><small>2019-05-30</small></sub>

**回归**一词的来由将在后面加以解释。在现实世界中存在着大量这样的情况：两个或多个变措之间有一些联系，但没有确切到可以严格决定的程度。

例如，人的身高X和体重Y有联系，一般表现为X大时，Y也倾向于大，但由X并不能严格地决定Y。一种农作物的面产量Y与其播种量$X_1$，施肥量 $X_2$有联系，但$X_1$ ,$X_2$ 不能严格决定Y。工业产品的质址指标Y与工艺参数和配方等有联系，但后者也不能严格决定Y。

在以上诸例及类似的例子中，Y通常称为**因变量**或**预报量**，$X,X_1,X_2$等则称为**自变量**或**预报因子**。因变量、自变量的称呼借用自函数关系，它不十分妥贴，因为，有时变量间并尤明显的因果关系存在。

---

现设在一个问题中有因变量$Y$，及自变量$X_1,X_2,\cdots,X_p$.可以设想$Y$的值由两部分构成：一部分由$X_1,X_2,\cdots,X_p$的影响所致，这一部分表为$X_1,X_2,\cdots,X_p$的函数形式$f(X_1,X_2,\cdots,X_p)$. 另一部分则由其他众多未加考虑的因素，包括随机因素的影响所致，它可视为一种随机误差，记为$e$，于是得到模型：

$$
Y = f(X_1,X_2,\cdots,X_p) + e
$$

e作为随机误差，我们要求其均值为0:

$$
E(e) = 0 
$$

于是得到：$f(X_1,X_2,\cdots,X_p)$就是在给定了自变量$X_1,X_2,\cdots,X_p$之值的条件下，因变量$Y$的条件期望值。可写为：

$$
f(X_1,X_2,\cdots,X_p) = E(Y|X_1,X_2,\cdots,X_p)
$$

函数$f(X_1,X_2,\cdots,X_p)$称为$Y$对$X_1,X_2,\cdots,X_p$的“**回归函数**”，而方程

$$
y = f(X_1,X_2,\cdots,X_p)
$$

则称为$Y$对$X_1,X_2,\cdots,X_p$的“**回归方程**”．有时在回归函数和回归方程之前加上“理论”二字，以表明它是直接来自模型，也可以说是模型的一个组成部分，而非由数据估计所得。后者称为 **经验回归函数** 和 **经验回归方程**。

设$\xi$为一随机变量，则$E(\xi-c)^2$作为$c$的函数，在$c=E(\xi)$ 处达到最小。由这个性质，可以对理论回归函数$f(X_1,X_2,\cdots,X_p)$作下面的斛释：如果我们只掌握了因素$X_1,X_2,\cdots,X_p$，而希望利用它们的值以尽可能好地逼近$Y$的值，则在均方误差最小的意义下，以使用理论回归函数为最好。

## 参考资料

- [概率论与数理统计 - 陈希孺](https://book.douban.com/subject/2201479/)