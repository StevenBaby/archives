# 计算机 cache 存储器

[annotation]: <id> (ebdf0ad2-dc01-47e6-b203-43d8d37baab7)
[annotation]: <status> (protect)
[annotation]: <create_time> (2019-04-22 11:46:41)
[annotation]: <category> (计算机科学)
[annotation]: <tags> (组成原理)

> 原文链接：<http://blog.ccyg.studio/article/ebdf0ad2-dc01-47e6-b203-43d8d37baab7>

---

以前在上学的时候，期末考试时，计算机组成原理这门课中 cache 存储器的分值占到了100分中40分，当时就意识到了这部分内容的重要性，但是对于计算机来说最重要的内容莫过于中央处理器了，但是中央处理器对于考试来说，不太好考，于是考试具有区分性的压力就来到了cache 存储器上面。

> CPU 在访问存储器时，无论是存取指令还是数据，所访问的存储单元都趋于聚集在一个较小的连续趋于中，这个原理叫做**局部性原理**

通常，在处理器即将要访问的主存位置极有可能是刚刚访问过的某个位置，所以高速缓存会自动保存一些来自近期使用过的副本。如果cache设计的合理，那么绝大多数的时候处理器所需要的存储器数据便已经在cache中。

计算机中存储器的限制可以归纳为3个问题：

- 容量有多大？
- 速度有多快？
- 价格有多贵？

计算机存储器的层次结构，或者说优先级吧，类似于下面整个列表：

1. 处理器内部寄存器
2. 高级缓存
3. 主存储器
4. 外部存储器
5. 可移动存储器

整个列表的优先级由高到低，价格也由高到低，访问速度也是由高到低。容量由低到高。

## cache 存储器原理

cache存储器的目的是使存储器的速度逼近可用的最快的存储器的速度，同时以较便宜的半导体存储器的价格提供一个大的存储器容量。

一般来说，一个相对大而慢的主存，加一个小而快的cache。cache 中存放了主存储器的部分副本。当CPU试图访问主存中的某个字时，首先检查这个字是否在 cache 中，如果是，则把这个字传送给CPU；如果不是，则将主存中包含这个字固定大小的块读入cache中，然后再将该字传送给CPU。因为访问的局部性原理，当把某一块数据存入cache，以满足某次访问存储器的访问时，CPU将来还有很大的可能接着访问同一个位置的数据。

一样可以使用多级cache，多级cache的使用，其中各级cache逐级访问速度递减，成本递减，容量递增。

## cache 的设计要素

### cache 地址

### cache 容量

我们希望 cache 的容量足够小，以至于整个存储系统的平均价格接近于单个主存储器的价格，同事我们也希望 cache 足够大，从而使得整个存储系统的平均存取时间接近于单个cache的存取时间。还有几个减小cache容量的动机。cache越大，寻址所需要电路门就会越多，结果是大的cache比小的稍慢，即使是采用相同的集成电路技术制造并放在芯片的电路板的同一位置。cache 容量也受芯片和电路板面积的限制，因为cache 的性能对工作负载的性能十分敏感，所以不可能有“最右”的cache容量。

## 映射功能

由于cache的行比主存储器的块要少，因此需要一种算法来实现主存块到cache行的映射。还需要一种方法来确定当前那一块占用了 cache 行，映射方法的选择决定了 cache 的组织结构，通常采用三种方法：

**直接映射**

直接映射是最简单的映射技术，将主存中的每个快映射到一个固定可用的cache 行中，直接映射可表示为：

>$$i = j\ mod\ m$$

其中i = cache 行号， j = 主存储器的块号，m = cache 的行数。

直接映射技术简单，实现花费少。主要缺点是：对于任意给定的块，它所对应的 cache 位置时固定的。因此，如果一个程序恰巧重复访问两个需要映射到同一行中且来自不同块的字，则这两个块将不断地被交换到cache中。cache的命中率将会降低（一种所谓的抖动现象）

**全相联映射**

全相连映射克服了直接映射的缺点，它允许每一个主存块装入cache中的任意行。这种情况下，cache 控制逻辑将存储地址简单地表示为一个标记域加一个字域。标记域用来唯一标识一个主存块。为了确定某块是否在 cache 中，cache 控制逻辑必须同时对每一行中的标记进行检查，看其是否匹配。地址中无对应行号的字段，所以cache中的行号不由地址格式决定。

对于全相联映射，当新的块读入 cache 中时，替换旧的一块很灵活。权相联映射的主要缺点时需要复杂的电路来并行检查所有的cache行标记。

主存中的一个地址可被映射进任意cache line，问题是：当寻找一个地址是否已经被cache时，需要遍历每一个cache line来寻找，这个代价很高。就像停车位可以大家随便停一样，停的时候简单，找车的时候需要一个一个停车位的找了。

全相联映射方式比较灵活，主存的各块可以映射到 cache 的任一块中，cache 的利用率高，块冲突概率低，只要淘汰 cache 中的某一块，即可调入主存的任一块。但是，由于 cache 比较电路的设计和实现比较困难，这种方式只适合于小容量 cache 采用。

**组相联映射**

组相连映射是一种折中的方法，它既体现了直接映射和全相联映射的有点，又避免了两者的缺点。

主存和Cache都分组，主存中一个组内的块数与Cache中的分组数相同，组间采用直接映射，组内采用全相联映射。主存块存放到哪个组是固定的，至于存到该组哪一块则是灵活的。即主存的某块只能映射到cache的特定组中的任意一块。

在组相联映射中，cache 分为 v 个组，每组包含k个行，他们的关系为：

$$
\begin{aligned}
m &= v \times k \\
i &= j\ mod\ v \\
\end{aligned}
$$

其中: i = cache 行号
j = 主存储器的块号
m = cache 的行数
v = 组数
k = 每组中的行数

这被称为 k 路组相联映射。

常采用的组相联结构 cache，每组内有2、4、8、16块，称为2路、4路、8路、16路组相联 cache。以上为2路组相联 cache。组相联结构 cache 是前两种方法的折中方案，适度兼顾二者的优点，尽量避免二者的缺点，因而得到普遍采用。

### 替换算法

一旦 cache 行被占用，当新的数据块装入 cache 中时，原存在的块必须被替换掉。对于直接映射，任意特殊块都只有唯一的一行可以使用，没有选择的可能。对于全相联和组相联映射技术，则需要一种替换算法。为了获得高速度，这种算法必须由硬件来实现。人们尝试过许多算法，下面介绍最常用的4中算法。

- **最近最少使用算法(LRU)**：这是最有效的算法，替换掉那些在cache中最长时间未被访问的块。对于二路组相联这种方法很容易实现，每行包含一个USE位，当某行被引用时，其USE位被置为1，而这一组中另一行的USE位被置为0。当把一块读入到这一组中时，就会替换掉USE位为0的行。由于我们假定越是最近使用的存储单元越有可能被访问，因此，LRU会给出最佳的命中率。多余全相联 cache，LRU也相对容易实现。告诉缓存机制会为cache中的每行保留一个单独的索引表。当某一行被访问时，它就会移动到表头，而在表尾的行将被替换掉。因为实现简单，LRU是目前使用最为广泛的替换算法。

- **先进先出算法(FIFO)**：替换掉在cache中停留时间最长的块，FIFO采用时间片轮转法或者环形缓冲技术很容易实现。

- **最不经常使用算法(LFU)**：替换掉 cache 中被访问次数最少的块，LFU可以用与每行相关的计数器来实现。

- **随机化算法**：在候选行中任意选取，然后替换。看似荒唐，但是实际实验表明，性能上只稍逊与基本使用情况的算法。


### 写策略

这里涉及到了，缓存同步的问题，当驻留在 cache中的某块要被替换时，必须考虑两点。如果 cache 中的原块没有被修改过，那么可以直接替换掉，而不需要事先写回主存。如果 cache 某行中至少在一个字上进行过写操作，那么在替换掉该块之前必须将该行写回主存对应的块，以进行主存更新，保证数据同步。

## 参考资料

- [计算机组成与体系结构：性能设计](https://book.douban.com/subject/6398113/)