# 高级人工智能

[annotation]: [id] (f59b6767-f0b4-4960-9fc8-37195acfbf64)
[annotation]: [status] (public)
[annotation]: [create_time] (2021-12-26 13:13:31)
[annotation]: [category] (计算机科学)
[annotation]: [tags] (人工智能|机器学习|强化学习|神经网络|数理逻辑)
[annotation]: [comments] (true)
[annotation]: [url] (http://blog.ccyg.studio/article/f59b6767-f0b4-4960-9fc8-37195acfbf64)

## 人工智能概述

> 智能：个体适应环境并能在不同环境中实现其目标的能力

概念性定义：

- 机器智能：使机器具备计算和判别的行为能力
- 类脑智能：仿生智能，让机器像人或生物一样思考
- 群体智能：社会智能的机器重现与利用、涌现智能

### 人工智能的起源

- 萌芽期：
    - 机械自动化
    - 逻辑推理
- 孕育期（文艺复兴以来）：
    - 理性主义
    - 数理逻辑学课
    - 计算思维：巴贝奇：差分机，图灵机
- 形成期(1956-1961)：
    - 1956 年，首次人工智能研讨会
    - IBM 的西洋跳棋程序，文法体系、逻辑推理机
- 发展期 (60 年代)：
    - 研究领域拓展
    - 1969 年，第一届国际人工智能联合会议 (IJCAI)
    - 1970 年，《人工智能》国际杂志创刊
- 寒冬期 (60s-70s)：
    - 1966 年，美国政府取消了机器翻译项目的所有投资
    - 英国政府取消了几乎所有人工智能研究投入
    - 神经网络的研究经费缩减到几乎没有
- 艰难前行 (70s)：
    - 弱方法：构建搜索机制，试图找出完全解，下棋：搜索解空间
    - 强方法：构建领域知识库 --> 专家系统
- 走向工业 (80s)：
    - 1982 年，第一个商用专家系统RI
    - 1981 年，日本启动“第五代计算机”计划，运行 prolog 语言的智能计算机
    - 美国、英国恢复对人工智能的投入
- 今天：
    - 大数据、计算能力提升、网络泛在化
    - 神经网络复兴：多层感知机，反向传播，隐马尔可夫模型(语音识别)，贝叶斯网络
    - 专家系统逐渐成熟，知识发现、数据挖掘兴起

### 图灵测试

### 达特茅斯会议

首次提出 **人工智能** 一词

### 人工智能三大学派

- 符号主义：逻辑学派，认为 **人的认知基元是符号，认知过程即符号操作过程**
    - 逻辑
    - 专家系统
    - 知识库
- 联接主义：仿生学派或生理学派，认为 **人的思维基元是神经元**
    - 人工神经网络
    - 认知科学
    - 类脑计算
- 行为主义：进化主义或控制论学派，认为 **智能取决于感知和行动**
    - 主张 **利用机器对环境作用后的相应或反馈为原型** 来实现智能
    - 控制论(维纳)
    - 多智能体
    - 强化学习

### 人工智能研究的课题

三大层次

- 基础理论：数学、思维科学、认知科学等
- 原理技术：启发式搜索、演化计算
- 工程应用：模式识别、计算机视觉、自然语言处理、问答系统

四大问题：

- 知识科学
- 问题求解
- 机器学习
- 系统构成

> 许多尖端的人工智能由于应用广泛，已经不再被称为人工智能。因为，人们一旦觉得某些东西非常有用并广泛使用，就不再称之为人工智能了。 —— 尼克·博斯特罗姆

## 搜索问题

搜索问题构成：

- 状态空间
- 后继函数
- 初始状态和目标测试

> **解** 是一个行为序列，将初始状态转换成目标状态

例子：野人过河问题

有三个传教士(Missionary) 和三个野人(Cannibal) 要过河，只有一条能装下两个人的船(Boat)，在河的任何一方或者船上，如果野人的人数大于传教士的人数，那么传教士就会有危险，你能不能找出一种或多种安全的渡河方法呢？

- 状态空间：$\{(M, C, B)\}$
- 后继函数：$\{P01, P10, P02, P20, P11, Q01, Q10, Q02, Q20, Q11\}$
- 初始状态：$(3, 3, 1)$
- 目标状态：$(0, 0, 0)$

其中后继函数表示 $Pmc$ 表示从左岸滑向右岸，$Qmc$ 表示从右岸滑向左岸。

状态空间图：

![](./images/mc_space.drawio.svg)

该问题的解：最短路径有 4 条，由 11 次操作构成

- (P11、Q10、P02、Q01、P20、Q11、P20、Q01、P02、Q01、P02)
- (P11、Q10、P02、Q01、P20、Q11、P20、Q01、P02、Q10、P11)
- (P02、Q01、P02、Q01、P20、Q11、P20、Q01、P02、Q01、P02)
- (P02、Q01、P02、Q01、P20、Q11、P20、Q01、P02、Q10、P11)

### 无信息搜索

**状态空间图和搜索树**

- 状态空间图中，每种状态只出现一次
- 扩展出潜在行动
- 维护所考虑行动的边缘结点
- 试图扩展尽可能少的树结点

**搜索算法特性**：

- 完备性：当问题有解时，保证能找到可行解
- 最优性：当问题有解时，保证能找到最优解
- 时间复杂度
- 空间复杂度

**深度优先搜索**

策略：利用栈处理边缘结点

- 时间复杂度：$O(b^m)$，其中 $b$ 为每次扩展的结点数，$m$ 为树的高度
- 空间复杂度：$O(bm)$
- 完备性：树高 $m$ 可能无穷大，除非我们防止循环搜索
- 最优性：不具备，找到的是最左侧的解，而不关心深度和代价

**广度优先搜索**

策略：利用队列处理边缘节点

- 时间复杂度：$O(b^d)$，其中 $b$ 为每次扩展的结点数，$d$ 为找到解的高度
- 空间复杂度：$O(b^d)$
- 完备性：如果解存在，那么 $d$ 必须是有限的，所以具备完备性
- 最优性：如果所有搜索代价都是相等的，那么是可以找到最优解的

**代价一致搜索**

策略：优先扩展代价最低的结点，数据结构优先队列

- 时间复杂度：$O(b^{C^* / \varepsilon})$，其中 $b$ 为每次扩展的结点数，$C^*$ 为找到解的代价，$\varepsilon$ 为最少的弧代价
- 空间复杂度：$O(b^{C^* / \varepsilon})$
- 完备性：具备
- 最优性：具备
- 缺点：在每一个方向上进行搜索，没有关于目标的信息。

### 启发式搜索

启发策略：

- 估计一个当前状态到目标距离的函数
- 问题给算法额外的信息，为特定的搜索问题设计特定的算法

启发函数：

- 通常，可采纳的启发函数是 **松弛问题** 的解的代价（耗散）

贪婪搜索：

- 策略：扩展离目标最近的结点
- 只是用启发函数 $f(n) = h(n)$ 来评价结点
- 通常情况：很快到达目标
- 最坏情况：类似于 DFS

$A^*$ 搜索：

- 策略：结合代价一致搜索和贪婪搜索
- 代价一致，为后向代价 $g(n)$
- 贪婪，为前向代价 $h(n)$
- 启发函数 $f(n) = g(n) + h(n)$
- 只有目标出队列时才停止

启发函数 $h$ 时可采纳的，那么必须满足：

$$0 \leqslant h(n) \leqslant h^*(n)$$

其中 $h^*$ 时到最近目标的真实代价，而 $h$ 时估计代价，想到启发函数 $h$ 时 $A^*$ 算法的重点。

$A^*$ 树搜索的最优性的证明

最优性：设 $A$ 是最优目标结点，$B$ 是次优目标结点，$h$ 是可采纳的代价函数，那么 $A$ 在 $B$ 之前离开边缘集合。

假设 $B$ 在边缘集合上，$A$ 的某个祖先结点 $A_p$ 也在边缘集合上，那么有 $f(A_p) \leqslant f(A)$，

由于 $A$ 是最优解，而 $B$ 是次优解，所以有 $f(A_p) \leqslant f(A) < f(B)$；

归纳地可得，$A$ 的所有祖先结点在 $B$ 之前扩展，$\Rightarrow$ $A$ 在 $B$ 之前扩展；

故 $A^*$ 是最优的；

### 局部搜索

局部搜索：改进单一选项直到不能再改善为止，没有边缘结合

新的后继函数：局部改变

通常更快，内存使用更有效，但不完备，次优，对于很难找到最优解的问题，通常次优解也是不错的选择。

爬山法：

- 可在任意位置开始
- 重复：移动到最好的相邻状态
- 如果没有比当前更好的相邻状态，则结束

模拟退火算法：

- 避免局部极大值（允许向山下移动）
- 静态分布 $p(x) \propto e^{E(x) \over kT}$
- 如果温度下降的足够慢，将收敛到最优解

遗传算法：

- 基于 **适应度函数**，再每步中保留 $N$ 个最好状态
- 配对杂交操作
- 产生可选的变异

## 神经网络和深度学习

- 联接主义学派或生理学派
    - 认为人的思维基元是神经元，而不是符号处理过程
    - 认为人脑不同于电脑
- 核心：智能的本质是联接机制
- 原理：神经网络及神经网络间的联接机制和学习算法

所谓人工神经网络是基于模仿生物大脑的结构和功能而构成的一种信息处理系统。

### 生物学启示

![](./images/neural_network.jpg)

- 神经元的组成：
    - 细胞体
    - 轴突
    - 树突
    - 突触
- 神经元之间通过突触两两相连，信息的传递通过动作电位发生在突触；
- 突触记录了神经元间联系的强弱；
- 只有达到一定的兴奋程度，神经元才向外输出信息；

TODO...

---

M-P 模型 <sup>[[ref]](#mp)</sup>：

$y = f(\xi) = f(g(x))$

其中：

- $f$ 为激活函数 (Activation Function)
- $g$ 为组合函数 (Combination Function)

加权和

$\displaystyle \xi = g(x) = \sum_{i = 1}^n w_ix_i - \theta$

其中：

- $w_i$ 为权重
- $\theta$ 为偏置，一般用 $b$ (bias) 表示

----

激活函数：

Sigmoid 函数 

$$\text{sigmoid}(\xi) = {1 \over 1 + e^{-\alpha \xi}}$$

ReLU 函数

$$\text{relu}(\xi) = \max\{0, \xi \}$$

多个神经元按照特定的网络结构联接在一起，就构成了一个人工神经网络，神经网络的目标就是将输入转换成有意义的输出。

### 感知机

> 感知机收敛定理：若训练集是线性可分的，则感知机模型收敛；

证明：

记 $\hat{w} = [w^T b]^T$, $\hat{x} = [x^T 1]^T$，则分离的超平面为 $\hat{w}^T \hat{x} = 0$

如果数据集线性可分，那么存在 $\hat{w}^*(\parallel \hat{w}^* \parallel = 1), \gamma > 0$，使得 $y_t \hat{w}^* x_t \geqslant \gamma$

令最终的分离超平面参数为 $\hat{w}^*$ 且其范数为 $1$

$$\begin{aligned}
\hat{w}_k \hat{w}^* =& (\hat{w}_{k - 1} + \hat{x}_t y_t) \hat{w}^* \geqslant \hat{w}_{k - 1} \hat{w}^* + \gamma \geqslant \cdots \geqslant k \gamma \\
\parallel \hat{w}_k \parallel^2 =& \parallel \hat{w}_{k - 1} + \hat{x}_t y_t \parallel^2 = \parallel \hat{w}_{k - 1} \parallel^2 + 2 \hat{w}_{k-1}^T \hat{x}_ty_t + \parallel \hat{x}_t \parallel^2 \\
\leqslant & \parallel \hat{w}_{k - 1} \parallel^2 + \parallel \hat{x}_t \parallel^2 \\
\leqslant & \parallel \hat{w}_{k - 1} \parallel^2 + R^2 \\
\leqslant & \cdots \\
\leqslant & kR^2 \\
\end{aligned}$$

于是 $k\gamma \leqslant \hat{w}_k \cdot \hat{w}^* \leqslant \parallel \hat{w}_k \parallel \cdot \parallel \hat{w}^* \parallel \leqslant \sqrt{k}R$ 

故 $\displaystyle k \leqslant {R^2 \over \gamma^2}$

解读：给定训练集 $D = \{(x^{(n)}, y^{(n)})\}_{n = 1}^N$，令 $R$ 是训练集中最大的特征向量的模，即：


$$R = \max_n \parallel x^{(n)} \parallel$$

如果训练集 $D$ 线性可分，两类感知机的参数学习算法的权重更新次数不会超过 $\displaystyle {R^2 \over \gamma^2}$

---

反向传播算法

均方误差函数 

梯度下降

梯度消失

### 深度学习

- 自动编码器
- Hopfield 网络
- 玻尔兹曼机

---

受限玻尔兹曼机 Restricted Boltzmann Machine (RBM)

----

深度信念网络 Deep Belief Networks(DBN)

深度信念网络（DBN）是一种多层网络，每两层网络受限于波兹曼机（RBM）。可以假设 DBN 是由众多 RBM 堆积而成的。

在 DBN 中，每个隐含层可以理解为是输入特征的抽象表示。它执行分类任务，其输出层是不同的。在 DBN 中可执行两个任务，无监督预训练和监督微调。

无监督预训练：在 RBM 中训练以重建输入。每一层接收来自前一层的输入。

监督微调：在这个过程中，利用标签和反向传播算法的梯度下降进行训练。

---

Deep Boltzmann Machine(DBM)

## 循环神经网络

循环神经网络 (Recurrent Neutral Networks, RNN)，学习序列数据，常常需要转换输入序列到不同领域的输出序列

GRU 单元 (Gated Recurrent Unit)

LSTM

BRNN

Deep RNN

## 卷积神经网络

卷积神经网络是一种特殊的深层神经网络模型

- 它的神经元间的连接是非全连接的
- 同一层中某些神经元之间的连接的权重是共享的

局部感受野：

- 图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱
- 减少了需要训练的权值数目


参数共享

卷积

边缘填充

步长

池化

Dropout

### 卷积神经网络实例

- ImageNet
- VGG Net
- ResNets
- Inception Net
- GoogleLeNet

## 生成对抗神经网络

- 生成器
- 判别器
- 纳什均衡

## 图神经网络

## 命题逻辑的语义推论

- 知识库(Knowledge Base, KB)：形式语言句子的集合；

1. 声明式的构建一个系统，将想要知道的都告诉它；
2. 然后问它问题，答案将会从知识库中得到；
3. 使得推理变成一个机械的过程；

- **逻辑**：表示信息以便得出结论的形式语言
- **语法**：定义语言中的句子
- **语义**：定义句子的含义；

----

### 逻辑研究的内容

- 语义：蕴含 entailment，逻辑推导
- 语法：演绎 inference，形式推演

![](./images/logic_01.drawio.svg)

- 可靠性：若句子 $S$ 可以用逻辑规则推演得到，那么 $S$ 为真
- 完备性：若句子 $S$ 为真，那么 $S$ 一定可以用逻辑规则推演得到
- 蕴含：$KB\vDash \alpha$，可以理解为 $KB$ 是 $\alpha$ 的充分条件，也可以理解为 $KB$ 是 $\alpha$ 的子集；

### 命题逻辑

- 命题：一个可以判断真假的句子
- 原子命题：不包含其他命题作为其组成部分的命题
- 文字：原子命题或者原子命题的否定

---

**命题逻辑的语法**

设 $S, S_1, S_2$ 为句子，那么

- 否定：$\neg S$ 为非 $S$
- 合取：$S_1 \wedge S_2$ 为 $S_1$ 或 $S_2$
- 析取：$S_1 \vee S_2$ 为 $S_1$ 与 $S_2$
- 蕴涵：$S_1 \Rightarrow S_2$，为如果 $S_1$，则 $S_2$
- 等价：$S_1 \Leftrightarrow S_2$，为 $S_1$ 和 $S_2$ 等价

**语义逻辑等价**

$$\begin{aligned}
A \wedge B &\equiv B \wedge A \\
A \vee B &\equiv B \vee A \\
(A \wedge B) \wedge C &\equiv A \wedge (B \wedge C) \\
(A \vee B) \vee C &\equiv A \vee (B \vee C) \\
\neg (\neg A) &\equiv A \\
A \Rightarrow B &\equiv \neg B \Rightarrow \neg A \\
A \Rightarrow B &\equiv \neg A \vee B \\
A \Leftrightarrow B &\equiv (A \Rightarrow B) \wedge (B \Rightarrow A) \\
\neg(A \wedge B) &\equiv \neg A \vee \neg B \\
\neg(A \vee B) &\equiv \neg A \wedge \neg B \\
A \wedge (B \vee C) &\equiv (A \wedge B) \vee (A \wedge C) \\
A \vee (B \wedge C) &\equiv (A \vee B) \wedge (A \vee C) \\
\end{aligned}$$

-----

- 真假赋值：是以所有命题符号的集为定义域，以真假值的集 $\{1,0\}$ 为值域的映射；
- 可满足性：$\Sigma$ 是可满足的，当且仅当有真假赋值 $v$ ，使得 $\Sigma^v = 1$，当 $\Sigma^v = 1$ 时，称 $v$ 满足 $\Sigma$；
- 重言式：$A$ 是重言式，当且仅当对于任意真假赋值 $v$ 都有 $A^v = 1$；
- 矛盾式：$A$ 是矛盾式，当且仅当对于任意真假赋值 $v$ 都有 $A^v = 0$；

## 命题逻辑的形式推演

### 形式推演的规则

----

合取范式：形如 $C_1 \wedge \cdots \wedge C_m$ 的式子，其中子句 $C_i$ 的形式为 $l_1 \vee \cdots \vee l_k$

**归结原理** <sup>[[ref]](#resolution)</sup>

$$l_1 \vee \cdots \vee l_k, \qquad m_1 \vee \cdots \vee m_n \over l_1 \vee \cdots \vee l_{i - 1} \vee l_{i + 1} \cdots  \vee l_k \vee m_1 \vee \cdots \vee m_{j - 1} \vee m_{j + 1} \vee \cdots \vee m_n$$

其中 $l_i$ 与 $m_j$ 是互补的文字，也就是说 $l_i \equiv \neg m_j$；

> 归结原理对命题逻辑是及 **可靠(sound)** 又 **完备(complete)** 的；

---

证明：若 $\Sigma \not\vdash \varnothing$, $\Sigma \vdash \alpha$ 当且仅当 $\{\Sigma, \neg \alpha \} \vdash \varnothing$，其中 $\vdash$ 仅使用归结原理获得新子句；

---

Modus Ponens 规则：

$$a_1, \cdots, a_n, \quad a_1, \cdots, a_n \Rightarrow b \over b$$

----

证明：Modus Ponens 规则是可靠的，即

$$a_1 \wedge \cdots \wedge a_n \wedge ( a_1 \wedge \cdots \wedge a_n \Rightarrow b) \vDash b$$

证明：

1. 构造如下真值赋值(指派 assignment) $m$，对任意文字 $a$，$a$ 赋值为 `true`，当且仅当 $a \in RC(KB)$ 

todo

---

## 一阶谓词逻辑

- 对象
- 关系
- 函数

----

- 常量
- 谓词
- 函数 
- 变量
- 连接词
- 等价
- 量词


## 模糊知识表达推理

## 知识表示学习

## 演化计算

## 强化学习

## 博弈

## 统计中的因果推断

### 为什么学习因果推断？

克服传统方法的不足

- 相关不意味着因果
- 缺少从数据中解读因果关系的有效数学语言或工具
- 统计无法回答反事实问题

### 辛普森悖论

**总体数据** 上得出的统计结论和 **分组数据** 上的统计结论相反

## 参考文献

1. <t id='mp'/> McClloch and Pitts, A logical calculus of the ideas immanent in nervous activity, 1943
2. <t id='infer'/>Judea Pearl, Madelyn Glymour, Nicholas P. Jewell Causal Inference in Statistics, Wiley 2016
3. <t id='resolution'/> J. A. Robinson. A machine-oriented logic based on the resolution principle.  Journal of the ACM, 1965, 12(1):23-41
