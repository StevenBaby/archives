# 高级人工智能

[annotation]: [id] (f59b6767-f0b4-4960-9fc8-37195acfbf64)
[annotation]: [status] (public)
[annotation]: [create_time] (2021-12-26 13:13:31)
[annotation]: [category] (计算机科学)
[annotation]: [tags] (人工智能|机器学习|强化学习|神经网络|数理逻辑)
[annotation]: [comments] (true)
[annotation]: [url] (http://blog.ccyg.studio/article/f59b6767-f0b4-4960-9fc8-37195acfbf64)

## 人工智能概述

> 智能：个体适应环境并能在不同环境中实现其目标的能力

概念性定义：

- 机器智能：使机器具备计算和判别的行为能力
- 类脑智能：仿生智能，让机器像人或生物一样思考
- 群体智能：社会智能的机器重现与利用、涌现智能

### 人工智能的起源

- 萌芽期：
    - 机械自动化
    - 逻辑推理
- 孕育期（文艺复兴以来）：
    - 理性主义
    - 数理逻辑学课
    - 计算思维：巴贝奇：差分机，图灵机
- 形成期(1956-1961)：
    - 1956 年，首次人工智能研讨会
    - IBM 的西洋跳棋程序，文法体系、逻辑推理机
- 发展期 (60 年代)：
    - 研究领域拓展
    - 1969 年，第一届国际人工智能联合会议 (IJCAI)
    - 1970 年，《人工智能》国际杂志创刊
- 寒冬期 (60s-70s)：
    - 1966 年，美国政府取消了机器翻译项目的所有投资
    - 英国政府取消了几乎所有人工智能研究投入
    - 神经网络的研究经费缩减到几乎没有
- 艰难前行 (70s)：
    - 弱方法：构建搜索机制，试图找出完全解，下棋：搜索解空间
    - 强方法：构建领域知识库 --> 专家系统
- 走向工业 (80s)：
    - 1982 年，第一个商用专家系统RI
    - 1981 年，日本启动“第五代计算机”计划，运行 prolog 语言的智能计算机
    - 美国、英国恢复对人工智能的投入
- 今天：
    - 大数据、计算能力提升、网络泛在化
    - 神经网络复兴：多层感知机，反向传播，隐马尔可夫模型(语音识别)，贝叶斯网络
    - 专家系统逐渐成熟，知识发现、数据挖掘兴起

### 图灵测试

### 达特茅斯会议

首次提出 **人工智能** 一词

### 人工智能三大学派

- 符号主义：逻辑学派，认为 **人的认知基元是符号，认知过程即符号操作过程**
    - 逻辑
    - 专家系统
    - 知识库
- 联接主义：仿生学派或生理学派，认为 **人的思维基元是神经元**
    - 人工神经网络
    - 认知科学
    - 类脑计算
- 行为主义：进化主义或控制论学派，认为 **智能取决于感知和行动**
    - 主张 **利用机器对环境作用后的相应或反馈为原型** 来实现智能
    - 控制论(维纳)
    - 多智能体
    - 强化学习

### 人工智能研究的课题

三大层次

- 基础理论：数学、思维科学、认知科学等
- 原理技术：启发式搜索、演化计算
- 工程应用：模式识别、计算机视觉、自然语言处理、问答系统

四大问题：

- 知识科学
- 问题求解
- 机器学习
- 系统构成

> 许多尖端的人工智能由于应用广泛，已经不再被称为人工智能。因为，人们一旦觉得某些东西非常有用并广泛使用，就不再称之为人工智能了。 —— 尼克·博斯特罗姆

## 搜索问题

搜索问题构成：

- 状态空间
- 后继函数
- 初始状态和目标测试

> **解** 是一个行为序列，将初始状态转换成目标状态

例子：野人过河问题

有三个传教士(Missionary) 和三个野人(Cannibal) 要过河，只有一条能装下两个人的船(Boat)，在河的任何一方或者船上，如果野人的人数大于传教士的人数，那么传教士就会有危险，你能不能找出一种或多种安全的渡河方法呢？

- 状态空间：$\{(M, C, B)\}$
- 后继函数：$\{P01, P10, P02, P20, P11, Q01, Q10, Q02, Q20, Q11\}$
- 初始状态：$(3, 3, 1)$
- 目标状态：$(0, 0, 0)$

其中后继函数表示 $Pmc$ 表示从左岸滑向右岸，$Qmc$ 表示从右岸滑向左岸。

状态空间图：

![图 {index}. 状态空间图](./images/mc_space.drawio.svg)

该问题的解：最短路径有 4 条，由 11 次操作构成

- (P11、Q10、P02、Q01、P20、Q11、P20、Q01、P02、Q01、P02)
- (P11、Q10、P02、Q01、P20、Q11、P20、Q01、P02、Q10、P11)
- (P02、Q01、P02、Q01、P20、Q11、P20、Q01、P02、Q01、P02)
- (P02、Q01、P02、Q01、P20、Q11、P20、Q01、P02、Q10、P11)

### 无信息搜索

**状态空间图和搜索树**

- 状态空间图中，每种状态只出现一次
- 扩展出潜在行动
- 维护所考虑行动的边缘结点
- 试图扩展尽可能少的树结点

**搜索算法特性**：

- 完备性：当问题有解时，保证能找到可行解
- 最优性：当问题有解时，保证能找到最优解
- 时间复杂度
- 空间复杂度

**深度优先搜索**

策略：利用栈处理边缘结点

- 时间复杂度：$O(b^m)$，其中 $b$ 为每次扩展的结点数，$m$ 为树的高度
- 空间复杂度：$O(bm)$
- 完备性：树高 $m$ 可能无穷大，除非我们防止循环搜索
- 最优性：不具备，找到的是最左侧的解，而不关心深度和代价

**广度优先搜索**

策略：利用队列处理边缘节点

- 时间复杂度：$O(b^d)$，其中 $b$ 为每次扩展的结点数，$d$ 为找到解的高度
- 空间复杂度：$O(b^d)$
- 完备性：如果解存在，那么 $d$ 必须是有限的，所以具备完备性
- 最优性：如果所有搜索代价都是相等的，那么是可以找到最优解的

**代价一致搜索**

策略：优先扩展代价最低的结点，数据结构优先队列

- 时间复杂度：$O(b^{C^* / \varepsilon})$，其中 $b$ 为每次扩展的结点数，$C^*$ 为找到解的代价，$\varepsilon$ 为最少的弧代价
- 空间复杂度：$O(b^{C^* / \varepsilon})$
- 完备性：具备
- 最优性：具备
- 缺点：在每一个方向上进行搜索，没有关于目标的信息。

### 启发式搜索

启发策略：

- 估计一个当前状态到目标距离的函数
- 问题给算法额外的信息，为特定的搜索问题设计特定的算法

启发函数：

- 通常，可采纳的启发函数是 **松弛问题** 的解的代价（耗散）

贪婪搜索：

- 策略：扩展离目标最近的结点
- 只是用启发函数 $f(n) = h(n)$ 来评价结点
- 通常情况：很快到达目标
- 最坏情况：类似于 DFS

$A^*$ 搜索：

- 策略：结合代价一致搜索和贪婪搜索
- 代价一致，为后向代价 $g(n)$
- 贪婪，为前向代价 $h(n)$
- 启发函数 $f(n) = g(n) + h(n)$
- 只有目标出队列时才停止

启发函数 $h$ 时可采纳的，那么必须满足：

$$0 \leqslant h(n) \leqslant h^*(n)$$

其中 $h^*$ 时到最近目标的真实代价，而 $h$ 时估计代价，想到启发函数 $h$ 时 $A^*$ 算法的重点。

$A^*$ 树搜索的最优性的证明

最优性：设 $A$ 是最优目标结点，$B$ 是次优目标结点，$h$ 是可采纳的代价函数，那么 $A$ 在 $B$ 之前离开边缘集合。

假设 $B$ 在边缘集合上，$A$ 的某个祖先结点 $A_p$ 也在边缘集合上，那么有 $f(A_p) \leqslant f(A)$，

由于 $A$ 是最优解，而 $B$ 是次优解，所以有 $f(A_p) \leqslant f(A) < f(B)$；

归纳地可得，$A$ 的所有祖先结点在 $B$ 之前扩展，$\Rightarrow$ $A$ 在 $B$ 之前扩展；

故 $A^*$ 是最优的；

### 局部搜索

局部搜索：改进单一选项直到不能再改善为止，没有边缘结合

新的后继函数：局部改变

通常更快，内存使用更有效，但不完备，次优，对于很难找到最优解的问题，通常次优解也是不错的选择。

爬山法：

- 可在任意位置开始
- 重复：移动到最好的相邻状态
- 如果没有比当前更好的相邻状态，则结束

模拟退火算法：

- 避免局部极大值（允许向山下移动）
- 静态分布 $p(x) \propto e^{E(x) \over kT}$
- 如果温度下降的足够慢，将收敛到最优解

遗传算法：

- 基于 **适应度函数**，再每步中保留 $N$ 个最好状态
- 配对杂交操作
- 产生可选的变异

## 神经网络和深度学习

- 联接主义学派或生理学派
    - 认为人的思维基元是神经元，而不是符号处理过程
    - 认为人脑不同于电脑
- 核心：智能的本质是联接机制
- 原理：神经网络及神经网络间的联接机制和学习算法

所谓人工神经网络是基于模仿生物大脑的结构和功能而构成的一种信息处理系统。

### 生物学启示

![图 {index}. 神经细胞的启示](./images/neural_network.jpg)

- 神经元的组成：
    - 细胞体
    - 轴突
    - 树突
    - 突触
- 神经元之间通过突触两两相连，信息的传递通过动作电位发生在突触；
- 突触记录了神经元间联系的强弱；
- 只有达到一定的兴奋程度，神经元才向外输出信息；

TODO...

---

M-P 模型 <sup>[[ref]](#mp)</sup>：

$y = f(\xi) = f(g(x))$

其中：

- $f$ 为激活函数 (Activation Function)
- $g$ 为组合函数 (Combination Function)

加权和

$\displaystyle \xi = g(x) = \sum_{i = 1}^n w_ix_i - \theta$

其中：

- $w_i$ 为权重
- $\theta$ 为偏置，一般用 $b$ (bias) 表示

----

激活函数：

Sigmoid 函数 

$$\text{sigmoid}(\xi) = {1 \over 1 + e^{-\alpha \xi}}$$

ReLU 函数

$$\text{relu}(\xi) = \max\{0, \xi \}$$

多个神经元按照特定的网络结构联接在一起，就构成了一个人工神经网络，神经网络的目标就是将输入转换成有意义的输出。

### 感知机

> 感知机收敛定理：若训练集是线性可分的，则感知机模型收敛；

证明：

记 $\hat{w} = [w^T b]^T$, $\hat{x} = [x^T 1]^T$，则分离的超平面为 $\hat{w}^T \hat{x} = 0$

如果数据集线性可分，那么存在 $\hat{w}^*(\parallel \hat{w}^* \parallel = 1), \gamma > 0$，使得 $y_t \hat{w}^* x_t \geqslant \gamma$

令最终的分离超平面参数为 $\hat{w}^*$ 且其范数为 $1$

$$\begin{aligned}
\hat{w}_k \hat{w}^* =& (\hat{w}_{k - 1} + \hat{x}_t y_t) \hat{w}^* \geqslant \hat{w}_{k - 1} \hat{w}^* + \gamma \geqslant \cdots \geqslant k \gamma \\
\parallel \hat{w}_k \parallel^2 =& \parallel \hat{w}_{k - 1} + \hat{x}_t y_t \parallel^2 = \parallel \hat{w}_{k - 1} \parallel^2 + 2 \hat{w}_{k-1}^T \hat{x}_ty_t + \parallel \hat{x}_t \parallel^2 \\
\leqslant & \parallel \hat{w}_{k - 1} \parallel^2 + \parallel \hat{x}_t \parallel^2 \\
\leqslant & \parallel \hat{w}_{k - 1} \parallel^2 + R^2 \\
\leqslant & \cdots \\
\leqslant & kR^2 \\
\end{aligned}$$

于是 $k\gamma \leqslant \hat{w}_k \cdot \hat{w}^* \leqslant \parallel \hat{w}_k \parallel \cdot \parallel \hat{w}^* \parallel \leqslant \sqrt{k}R$ 

故 $\displaystyle k \leqslant {R^2 \over \gamma^2}$

解读：给定训练集 $D = \{(x^{(n)}, y^{(n)})\}_{n = 1}^N$，令 $R$ 是训练集中最大的特征向量的模，即：


$$R = \max_n \parallel x^{(n)} \parallel$$

如果训练集 $D$ 线性可分，两类感知机的参数学习算法的权重更新次数不会超过 $\displaystyle {R^2 \over \gamma^2}$

---

反向传播算法

均方误差函数 

梯度下降

梯度消失

### 深度学习

- 自动编码器
- Hopfield 网络
- 玻尔兹曼机

---

受限玻尔兹曼机 Restricted Boltzmann Machine (RBM)

----

深度信念网络 Deep Belief Networks(DBN)

深度信念网络（DBN）是一种多层网络，每两层网络受限于波兹曼机（RBM）。可以假设 DBN 是由众多 RBM 堆积而成的。

在 DBN 中，每个隐含层可以理解为是输入特征的抽象表示。它执行分类任务，其输出层是不同的。在 DBN 中可执行两个任务，无监督预训练和监督微调。

无监督预训练：在 RBM 中训练以重建输入。每一层接收来自前一层的输入。

监督微调：在这个过程中，利用标签和反向传播算法的梯度下降进行训练。

---

Deep Boltzmann Machine(DBM)

## 循环神经网络

循环神经网络 (Recurrent Neutral Networks, RNN)，学习序列数据，常常需要转换输入序列到不同领域的输出序列

GRU 单元 (Gated Recurrent Unit)

LSTM

BRNN

Deep RNN

## 卷积神经网络

卷积神经网络是一种特殊的深层神经网络模型

- 它的神经元间的连接是非全连接的
- 同一层中某些神经元之间的连接的权重是共享的

局部感受野：

- 图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱
- 减少了需要训练的权值数目


参数共享

卷积

边缘填充

步长

池化

Dropout

### 卷积神经网络实例

- ImageNet
- VGG Net
- ResNets
- Inception Net
- GoogleLeNet

## 生成对抗神经网络

- 生成器
- 判别器
- 纳什均衡

## 图神经网络

## 命题逻辑的语义推论

- 知识库(Knowledge Base, KB)：形式语言句子的集合；

1. 声明式的构建一个系统，将想要知道的都告诉它；
2. 然后问它问题，答案将会从知识库中得到；
3. 使得推理变成一个机械的过程；

- **逻辑**：表示信息以便得出结论的形式语言
- **语法**：定义语言中的句子
- **语义**：定义句子的含义；

----

### 逻辑研究的内容

- 语义：蕴含 entailment，逻辑推导
- 语法：演绎 inference，形式推演

![图 {index}. 蕴含和推演的关系](./images/logic_01.drawio.svg)

- 可靠性：若句子 $S$ 可以用逻辑规则推演得到，那么 $S$ 为真
- 完备性：若句子 $S$ 为真，那么 $S$ 一定可以用逻辑规则推演得到
- 蕴含：$KB\vDash \alpha$，可以理解为 $KB$ 是 $\alpha$ 的充分条件，也可以理解为 $KB$ 是 $\alpha$ 的子集；

### 命题逻辑

- 命题：一个可以判断真假的句子
- 原子命题：不包含其他命题作为其组成部分的命题
- 文字：原子命题或者原子命题的否定

---

**命题逻辑的语法**

设 $S, S_1, S_2$ 为句子，那么

- 否定：$\neg S$ 为非 $S$
- 合取：$S_1 \wedge S_2$ 为 $S_1$ 或 $S_2$
- 析取：$S_1 \vee S_2$ 为 $S_1$ 与 $S_2$
- 蕴涵：$S_1 \Rightarrow S_2$，为如果 $S_1$，则 $S_2$
- 等价：$S_1 \Leftrightarrow S_2$，为 $S_1$ 和 $S_2$ 等价

**语义逻辑等价**

$$\begin{aligned}
A \wedge B &\equiv B \wedge A \\
A \vee B &\equiv B \vee A \\
(A \wedge B) \wedge C &\equiv A \wedge (B \wedge C) \\
(A \vee B) \vee C &\equiv A \vee (B \vee C) \\
\neg (\neg A) &\equiv A \\
A \Rightarrow B &\equiv \neg B \Rightarrow \neg A \\
A \Rightarrow B &\equiv \neg A \vee B \\
A \Leftrightarrow B &\equiv (A \Rightarrow B) \wedge (B \Rightarrow A) \\
\neg(A \wedge B) &\equiv \neg A \vee \neg B \\
\neg(A \vee B) &\equiv \neg A \wedge \neg B \\
A \wedge (B \vee C) &\equiv (A \wedge B) \vee (A \wedge C) \\
A \vee (B \wedge C) &\equiv (A \vee B) \wedge (A \vee C) \\
\end{aligned}$$

-----

- 真假赋值：是以所有命题符号的集为定义域，以真假值的集 $\{1,0\}$ 为值域的映射；
- 可满足性：$\Sigma$ 是可满足的，当且仅当有真假赋值 $v$ ，使得 $\Sigma^v = 1$，当 $\Sigma^v = 1$ 时，称 $v$ 满足 $\Sigma$；
- 重言式：$A$ 是重言式，当且仅当对于任意真假赋值 $v$ 都有 $A^v = 1$；
- 矛盾式：$A$ 是矛盾式，当且仅当对于任意真假赋值 $v$ 都有 $A^v = 0$；

## 命题逻辑的形式推演

### 形式推演的规则

----

合取范式：形如 $C_1 \wedge \cdots \wedge C_m$ 的式子，其中子句 $C_i$ 的形式为 $l_1 \vee \cdots \vee l_k$

**归结原理** <sup>[[ref]](#resolution)</sup>

$$l_1 \vee \cdots \vee l_k, \qquad m_1 \vee \cdots \vee m_n \over l_1 \vee \cdots \vee l_{i - 1} \vee l_{i + 1} \cdots  \vee l_k \vee m_1 \vee \cdots \vee m_{j - 1} \vee m_{j + 1} \vee \cdots \vee m_n$$

其中 $l_i$ 与 $m_j$ 是互补的文字，也就是说 $l_i \equiv \neg m_j$；

> 归结原理对命题逻辑是及 **可靠(sound)** 又 **完备(complete)** 的；

---

证明：若 $\Sigma \not\vdash \varnothing$, $\Sigma \vdash \alpha$ 当且仅当 $\{\Sigma, \neg \alpha \} \vdash \varnothing$，其中 $\vdash$ 仅使用归结原理获得新子句；

---

Modus Ponens 规则：

$$a_1, \cdots, a_n, \quad a_1, \cdots, a_n \Rightarrow b \over b$$

----

证明：Modus Ponens 规则是可靠的，即

$$a_1 \wedge \cdots \wedge a_n \wedge ( a_1 \wedge \cdots \wedge a_n \Rightarrow b) \vDash b$$

证明：

1. 构造如下真值赋值(指派 assignment) $m$，对任意文字 $a$，$a$ 赋值为 `true`，当且仅当 $a \in RC(KB)$ 

todo

---

## 一阶谓词逻辑

- 对象
- 关系
- 函数

----

- 常量
- 谓词
- 函数 
- 变量
- 连接词
- 等价
- 量词


## 模糊知识表达推理

## 知识表示学习

## 演化计算

## 群体智能

群体智能指的是 **无智能** 或者仅具有 **相对简单智能** 的 主体 通过 **合作涌现** 出更高智能行为的特性

单个复杂个体可以实现的功能，同样可以由大量简单的个体通过群体合作实现，后者的优势在于它更健壮、灵活和经济。

群体智能利用群体优势，在没有中心控制的条件下，寻找解决复杂问题的新思路。


- 集群智能：众多无智能的个体，通过相互之间的简单合作所表现出来的智能行为
- 博弈：具备一定智能的理性个体，按照某种机制行动，在群体层面体现出的智能
- 众包：设计合适的机制，激励个体参与，从而实现单个个体不具备的社会智能

集群智能是 **分布式**、**自组织** 的（自然/人造）系统表现出的一种群体智能

集群智能系统一般由一群简单的智能体构成，智能体按照简单的规则彼此进行局部交互，智能体也可以环境交互；

集群智能的特点：

- 分布式：无中心控制
- 随机性：非确定性
- 自适应：个体根据环境进行策略调整
- 正反馈：个体好的尝试会对个体产生正反馈
- 自发涌现：会在群体层面涌现出一种智能

### 蚁群优化算法

Ant Colony Optimization (ACO) <sup>[[ref]](#ant)</sup>：

- 一种解空间搜索方法
- 适用于在图上寻找最优路径

形式化：

- 每个蚂蚁对应一个计算智能体
- 蚂蚁依概率选择侯选位置进行移动
- 在经过的路径上留下 **信息素(Pheromone)**
- 信息素随时间挥发
- 信息素浓度大的路径在后续的选择中会以更高的概率被选取

-----

旅行商问题 (Traveling Salesman Problem, TSP)：

- $n$ 个城市的有向图 $G = (V, E)$
- 城市之间的距离表示为 $d_{ij}$，表示结点 $i$ 和 $j$ 之间的距离
- 目标函数 $\displaystyle f(w) = \sum_{l = 1}^n d_{i_l i_{l + 1}}$，$w = (i_1, i_2, \cdots, i_n)$，为旅行商问题的任意可行解，其中 $i_{n + 1} = i_1$

算法：

首先将 $m$ 只蚂蚁随机放置在 $n$ 个城市，位于城市 $i$ 的第 $k$ 只蚂蚁选择下一个城市 $j$ 的概率为：

$$p_{ij}^k(t) = \begin{cases}
{[\tau_{ij}(t)]^\alpha[\eta_{ij}(t)]^\beta \over \sum_{k \in \text{allowed}} [\tau_{ik}(t)]^\alpha [\eta_{ik}(t)]^\beta}, & j \in \text{allowed} \\
0, & \text{otherwise}
\end{cases} \quad (1)$$

其中：

- $\tau_{i, j}(t)$ 表示边 $(i, j)$ 上的信息素浓度；
- $\eta_{i, j}(t) = {1 \over d_{ij}}$，是根据距离定义的启发信息
- $\alpha, \beta$ 反映了信息素与启发信息的相对重要性。

当所有蚂蚁完成周游后，按一下公式进行信息素更新：

$$\begin{aligned}
\Delta \tau_{ij}^k &= f(x) = \begin{cases}
{Q \over L_k}, & (i, j) \in w_k \\
0, & \text{otherwise}
\end{cases}  \quad (2)\\
\tau_{ij} (t + 1) &= \rho \cdot \tau_{i j} (t) + \Delta \tau_{ij} \\
\Delta \tau_{ij} &= \sum_{k = 1}^m \Delta \tau_{ij}^k
\end{aligned}$$

其中：

- $Q$ 为常数
- $w_k$ 表示第 $k$ 只蚂蚁在本轮迭代中走过的路径
- $L_k$ 为路径长度
- $\rho$ 为小于 $1$ 的常数，反应信息素的挥发速度

TSP 问题蚁群算法流程：

```ruby
1. 初始化，随机防止蚂蚁
2. 迭代过程
k = 1
while k <= count do:
    for i = 1 to m do:
        for j = 1 to n - 1 do:
            根据式 (1) 采用轮盘赌方法在窗口外选择下一个城市 j;
            将 j 置入禁忌表，蚂蚁转移到 j;
        end for
    end for
    计算每只蚂蚁的路径长度;
    根据式(2)更新所有蚂蚁路径上的信息量;
    k += 1;
end while
3. 输出结果，结束算法;
```

- 蚁群大小：一般情况下，蚁群的蚂蚁个数不超过 TSP 图中结点的个数
- 终止条件：
    - 设定迭代轮数
    - 设定最优解连续保持不变的迭代轮数
- 思想：
    - 局部随机搜索 + 子增强
    - 鲁迅：世界上本无路，走的人多了便有了路；
- 缺点：
    - 收敛速度慢
    - 易于陷入局部最优
    - 对于解空间为连续的优化问题不适用
    - 图中有环可能会陷入环中，环中的信息素不断增强；

### 粒子群算法

粒子群优化算法是一种基于种群寻优的启发式搜索算法。在 1995 年由Kennedy 和 Eberhart 首先提出来的。

它的主要启发来源于对鸟群群体运动行为的研究。我们经常可以观察到鸟群表现出来的同步性，虽然每只鸟的运动行为都是互相独立的，但是在整个鸟群的飞行过程中却表现出了高度一致性的复杂行为，并且可以自适应的调整飞行的状态和轨迹。

鸟群具有这样的复杂飞行行为的原因，可能是因为每只鸟在飞行过程中都遵循了一定的行为规则，并能够掌握邻域内其它鸟的飞行信息。

粒子群优化算法借鉴了这样的思想，每个粒子代表待求解问题搜索解空间中的一个潜在解，它相当于一只鸟，“飞行信息”包括粒子当前的位置和速度两个状态量。

每个粒子都可以获得其邻域内其它个体的信息，对所经过的位置进行评价，并根据这些信息和位置速度更新规则，改变自身的两个状态量，在“飞行”过程中传递信息和互相学习，去更好地适应环境。

随着这一过程的不断进行，粒子群最终能够找到问题的近似最优解。

-----

PSO: Particle Swarm Optimization <sup>[[ref]](#particle)</sup>：

- 一种随机优化方法
- 通过粒子群在解空间中进行搜索，寻找最优解（适应度最大的解）

构成要素形式化：

- 粒子群：
    - 每个粒子对应求解问题的一个可行解
    - 粒子通过其位置和速度表示
    - 粒子 $i$ 在第 $n$ 轮的位置：$x_n^{(i)}$
    - 粒子 $i$ 在第 $n$ 轮的速度：$v_n^{(i)}$
- 记录：
    - $p_{best}^{(i)}$ 粒子 $i$ 的历史最好位置
    - $g_{best}$ 全局历史最好位置
- 计算适应度函数：$f(x)$

算法过程描述：

- 初始化:
    - 初始化粒子群：每个粒子的位置和速度，即 $x_n^{(i)}$ 和 $v_n^{(i)}$
    - $p_{best}^{(i)}$ 和 $g_{best}$
- 循环直至结束：
    - 计算每个粒子的适应度：$f(x_n^{(i)})$
    - 更新每个粒子历史最好适应度及其相应的位置
    - 更新当前全局最好适应度及其相应的位置
    - 以下列公式更新每个粒子的速度和位置

$$\begin{aligned}
v_{n + 1}^{(i)} &= v_n^{(i)} + c_1 \cdot r_1 \cdot \left( p_{best}^{(i)} - x_n^{(i)}\right) + c_2 \cdot r_2 \cdot \left( g_{best} - x_n^{(i)}\right)\\
x_{n + 1}^{(i)} &= x_n^{(i)} + v_{n + 1}^{(i)}
\end{aligned}$$

其中：

- $v_n^{(i)}$ 表示惯性，粒子会以原速度保持不变的倾向；
- $p_{best}^{(i)} - x_n^{(i)}$：记忆项，粒子回到历史最好位置的倾向；
- $g_{best} - x_n^{(i)}$：社会项，走向粒子群全局最好位置的倾向；
- $c_1, c_2$，为权重参数，一般取值为 $2$
- $r_1, r_2$，为 $[0, 1]$ 之间的随机数

算法终止条件：

- 迭代的轮数
- 最佳位置连续未更新的轮数
- 适应度函数的值达到预期要求

速度更新参数分析，又称加速度参数，用来控制粒子当前最优解位置 $p_{best}^{(i)}$ 和粒子全局最优解位置 $g_{best}$ 对飞行速度的影响；

- $c_1 > 0, c_2 = 0$：每个粒子执行局部搜索；
- $c_1 = 0, c_2 > 0$：算法转化为一个随机爬山法；
- $c_1 = c_2 > 0$：粒子逐渐移向 $\vec{p}_g$ 和 $\vec{p}_i$ 的加权均值；
- $c_1 < c_2$：算法比较适合于单峰优化问题；
- $c_1 > c_2$：算法比较适合于多峰优化问题；

和遗传算法相比：

- 遗传算法强调 **适者生存**，适应度低的个体在竞争中被淘汰；PSO 强调 **协同合作**，适应度低的个体通过学习适应度好的个体，向好的方向转变；
- 遗传算法中最好的个体通过产生更多的后代来传播基因；PSO 中的最好个体通过吸引其他个体向他靠拢来施加影响；
- 遗传算法的选择概率只与上一代群体相关，而与历史无关，群体的信息变化过程是一个马尔可夫链过程；而 PSO 的个体除了有位置和速度外，还有这过去的历史信息 $p_{best}, g_{best}$；

优点：

- 易于实现
- 可调参数较少
- 所需种群或微粒群规模较小；
- 计算效率高，收敛速度快；
- 适用于连续空间的求解；

缺点：和其他演化计算算法类似，不具有最优性；


## 强化学习

目标：学习 **从环境状态到行为的映射(策略)**，智能体选择能够获得环境**最大奖赏**的行为，使得外部环境对学习系统在某种意义下的评价为最佳；

- 监督学习是从**标注**中学习；
- 强化学习是从**交互**中学习；

两种反馈：

- 评价性反馈：Evaluative feedback  
    - 当智能体采取某个行动时，对该行为给出一个评价，但不知道哪个行为是最好的；
    - 强化学习使用评价性反馈
- 指导性反馈：Instructive feedback
    - 直接给出某个状态下的正确或最好的行为
    - 独立于智能体当前采取的行为
    - 监督学习使用的是指导性反馈

两大特征：用于判断某一问题是否适用于强化学习求解

- 试错搜索
- 延迟奖励


强化学习的要素：

- 主体：智能体和环境
    - 状态
    - 行为
    - 奖励
- 要素：
    - 策略：状态到行为的映射，包括确定策略和随机策略两种
    - 奖励：关于状态和行为的函数，通常具有一定的不确定性
    - 价值：累计奖励或长期目标
    - 环境模型：刻画环境对行为的反馈

![图 {index}. 智能体和环境的关系](./images/reinforcement_01.drawio.svg)

### 多臂赌博机

一台赌博机有**多个摇臂**，每个摇臂摇出的 **奖励(reward)** **大小不确定**，玩家希望摇 **固定次数** 的臂所获得的 **期望累积奖励最大**；

多臂赌博机形式化：

- 行为：摇哪个臂
- 奖励：每次摇臂获得的奖金
- $A_t$ 表示第 $t$ 轮的行为，$R_t$ 表示第 $t$ 轮获得的奖励
- 第 $t$ 轮采取行为 $a$ 的期望奖励为： $q_*(a) \doteq \mathbb{E}[R_t | A_t = a]$

----

贪心策略：

- 一般情况下，$q_*(a)$ 对玩家而言是未知的或具有不确定性；
- 玩家在 $t$ 轮时只能依赖于当时对 $q_*(a)$ 的估值 $Q_t(a)$ 进行选择；
- 此时，贪心策略在第 $t$ 轮选择 $Q_t(a)$ 最大的 $a$

利用和探索：

利用：Exploitation：

- 按照贪心策略进行选择，即选出 $Q_t(a)$ 最大的行为 $a$
- 优点：最大化即时奖励
- 缺点：由于 $Q_t(a)$ 只是对 $q_*(a)$ 的估计，估计的不确定性导致按照贪心策略选择的行为不一定是 $q_*(a)$ 最大的行为；

探索：Exploration

- 选择贪心策略之外的行为
- 缺点：短期奖励会比较低
- 优点：长期奖励会比较高，通过探索可以找到奖励更大的行为，供后续选择

每步选择在 利用 和 探索 中二选一，如何平衡 利用 和 探索 是关键；

----

贪心策略形式化地表示为：

$$A_t \doteq \underset{a}{\text{arg max }} Q_t(a)$$

当有多个行为的 $Q_t(a)$ 同时为最大时，随机选择一个；

---

$\varepsilon$ 贪心策略：

- 以概率 $1 - \varepsilon$ 按照贪心策略进行行为选择 - exploitation
- 以概率 $\varepsilon$ 在所有行为中随机选择一个 - exploration
- $\varepsilon$ 的取值取决于 $q_*(a)$ 的方差，方差越大 $\varepsilon$ 取值应越大；

---

行为估值方法：

根据历史观测样本的均值对 $q_*(a)$ 进行估计

$$Q_t(a) \doteq {\displaystyle \sum_{i = 1}^{t - 1} R_i \cdot 1_{A_i = a}  \over  \displaystyle \sum_{i = 1}^{t - 1} 1_{A_i = a}}$$

其中：

- 分母：表示在 $t$ 之前选择行为 $a$ 的次数
- 分子：表示在 $t$ 之前因行为 $a$ 获得的奖励的总和
- 约定：当分母为 0 时，$Q_t(a) = 0$
- 当分母区域无穷大时，$Q_t(a)$ 收敛到 $q_*(a)$

---

行为估值的实现：

行为估值时，一个行为被选择了 $n$ 次后的估值记为：

$$Q_n \doteq {R_1 + R_2 + \cdots + R_{n - 1} \over n - 1}$$

该估值方式需要记录 $n - 1$ 个奖励值

---

行为估值增量式实现：

$$\begin{aligned}
Q_{n + 1} =& {1 \over n} \sum_{i = 1}^n R_i \\
=& {1 \over n} \left(R_n + \sum_{i = 1}^{n - 1} R_i \right) \\
=& {1 \over n} \left(R_n + (n - 1) \cdot {1 \over n - 1}\sum_{i = 1}^{n - 1} R_i \right) \\
=& {1 \over n} (R_n + (n - 1) Q_n) \\
=& {1 \over n} (R_n + nQ_n - Q_n) \\
=& Q_n + {1 \over n} (R_n - Q_n) \\
\end{aligned}$$

于是，只需要记录 $Q_n$ 和 $n$ 两个值

----

更具一般性的行为估值：

$$新估计 \leftarrow 旧估计 + 步长 \times [目标 - 旧估计]$$

目标 - 旧估计 表示估计误差，

- 对于贪心策略增量实现而言：步长 $= {1 \over n}$
- 更一般的情况：步长用参数 $\alpha$ 或 $\alpha_t(a)$ 表示

---

平稳问题：

- $q_*(a)$ 是稳定的，不随实践变化
- 随着观测样本的增加，平均估值方法最终收敛于 $q_*(a)$

非平稳问题：

- $q_*(a)$ 是关于实践的函数
- 对 $q_*(a)$ 的估计需要更关注最近的观测样本

---

非平稳情形下的行为估值：

行为估值的更新公式：

$$Q_{n + 1} \doteq Q_n + \alpha [R_n - Q_n]$$

递推得到：


$$\begin{aligned}
Q_{n + 1} \doteq & Q_n + \alpha [R_n - Q_n] \\
=& \alpha R_n + (1 - \alpha)Q_n \\
=& \alpha R_n + (1 - \alpha) [\alpha R_{n - 1} + (1 - \alpha) Q_{n - 1}] \\
=& \alpha R_n + (1 - \alpha)\alpha R_{n - 1} + (1 - \alpha)^2\alpha R_{n - 2} + \\
& \cdots + (1 - \alpha)^{n - 1}\alpha R_{1} + (1 - \alpha)^n Q_1 \\
=& (1 - \alpha)^n Q_1 + \sum_{i = 1}^ n \alpha(1 - \alpha)^{n - i} R_i
\end{aligned}$$

更新步长的选择：

并不是所有的步长选择 $\{\alpha_n(a)\}$ 都保证收敛

- $\alpha_n(a) = {1 \over n}$ 收敛
- $\alpha_n(a) = a$ 不收敛

收敛条件

$$\sum_{n = 1}^\infty \alpha_n(a) = \infty \quad \text{and}\quad \sum_{n = 1}^\infty \alpha^2_n(a) < \infty$$

- 第一个条件保证步长足够大，克服初值或随机扰动的影响
- 第二个条件保证步长最终会越来越小，小到保证收敛

---

行为选择策略：

- 贪心策略：选择当前估值最好的行为
- $\varepsilon$ 贪心策略：以一定的概率随机选择非贪心行为，但是对于非贪心行为不加区分；
- 平衡 exploitation 和 exploration，应对行为估值的不确定性
- 关键：确定每个行为被选择的概率

乐观初值法：

- 为每个行为赋一个高的初始值
- 好处：初期每个行为都有较大的机会被选择

(Upper-Confidence-Bound, UCB) 行为选择策略：

$$A_t \doteq \underset{a}{\text{arg max }} \left[Q_t(a) + c\sqrt{\ln t \over N_t(a)}\right]$$

$N_t(a)$ 表示时刻 $t$ 之前行为 $a$ 被选择的次数；

公式解读：

- 选择潜力大的行为：依据估值的置信上界进行行为选择
- 第一项表示当前估值要高，接近贪心选择
- 第二项表示不确定性要高，被选择的次数少，估的就不准确，不确定性就高
- 参数 $c$ 用来控制探索的程度

UCB 策略一般会优于 $\varepsilon$ 贪心策略，不过最初几轮相对较差；

---

梯度赌博机算法：

和前面的确定策略不同，梯度赌博机是一种随机策略

- 使用 $H_t(a)$ 表示在第 $t$ 轮对行为 $a$ 的偏好程度
- 根据行为选择后获得的奖励大小更新 $H_t(a)$

在第 $t$ 轮选择行为 $a$ 的概率为

$$Pr\{A_t = a\} \doteq {e^{H_t(a)} \over \displaystyle \sum_{b = 1}^k e^{H_t(b)} } \doteq \pi_t(a)$$

更新公式

$$\begin{aligned}
H_{t + 1}(A_t) \doteq & H_t(A_t) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(A_t)) \\
H_{t + 1}(a) \doteq & H_t(a) - \alpha(R_t - \overline{R}_t) \pi_t(a) \\
\end{aligned}$$

等价于随机梯度上升

优化目标：第𝑡轮的期望奖励大小

$$\mathbb{E}[R_t] = \sum_b \pi_t(b) q_*(b)$$

### 马尔科夫决策过程

形式化记号:

- 智能体在时间步 $t$ 所处的状态记为 $S_t \in S$
- 智能体在时间步𝑡所采取的行为记为 $A_t \in A(s)$
- 采取行为 $A_t$ 后智能体转到状态 $S_{t + 1}$，并获得奖励 $R_{t + 1} \in R$
- 马尔科夫决策过程产生的序列记为

$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \cdots$$


----

有限马尔科夫决策过程的建模

模型

$$p(s', r | s, a) \doteq Pr\{ S_t = s', R_t = r | S_{t -1} = s, A_{t - 1} = a\}$$

这里 $p: S \times R \times S \times A \to [0, 1]$，满足s

$$\sum_{s' \in S}\sum_{r \in R} p(s', r | s, a) = 1, \quad \text{for all } s \in S, a \in A(s)$$

表示所有的概率和为 $1$

状态转移概率：

$$p(s' | s, a) \doteq Pr\{ S_t = s' | S_{t -1} = s, A_{t - 1} = a\} = \sum_{r \in R} p(s', r | s, a) $$

状态-行为 对期望的奖励：

$$r(s, a) \doteq \mathbb{E}[R_t | S_{t - 1} = s, A_{t - 1} = a] = \sum_{r \in R} r \sum_{s' \in S} p(s', r | s, a)$$

状态-行为-下一状态的奖励：

$$r(s,a,s') \doteq [R_t | S_{t - 1} = s, A_{t - 1} = a, S_t = s'] = \sum_{r \in R} r {p(s', r|s, a) \over p(s' | s, a)}$$

----

累计奖励：

$$G_t \doteq R_{t + 1} + R_{t + 2} + \cdots + R_T$$

$T$ 表示最后一步，对应的状态被称为终止态；

具有终止态的马尔科夫决策过程被称为 **多幕式** 任务

没有终止态的任务称之为连续式任务，其累计奖励记为：

$$G_t \doteq R_{t + 1} + \gamma R_{t + 2} + \cdots = \sum_{k = 0}^ \infty \gamma^k R_{t + k + 1}$$

其中：$0 \leqslant \gamma \leqslant 1$ 表示折扣率（衰减因子）

----

累计奖励的递推公式：

$$\begin{aligned}
G_t \doteq& R_{t + 1} + \gamma R_{t + 2} + \gamma^2 R_{t + 3} + \gamma^3 R_{t + 4} + \cdots \\
=& R_{t + 1} + \gamma (R_{t + 2} + \gamma R_{t + 3} + \gamma^2 R_{t + 4} + \cdots )\\
=& R_{t + 1} + \gamma G_{t + 1}\\
\end{aligned}$$

求和公式：

$$G_t \doteq \sum_{k=t + 1}^T \gamma^{k - t - 1} R_k$$

注意：$\gamma = 1$ 和 $T = \infty$ 不能同时出现；

### 贝尔曼方程

策略和状态估值函数

策略：

- 状态到行为的映射
- 随机式策略：$\pi(a|s)$
- 确定式策略：$a = \pi(s)$

给定策略 $\pi$，状态估值函数(State Value Function)定义为：

$$v_\pi(s) \doteq \mathbb{E}_\pi[G_t | S_t = s] = E_\pi\left[\sum_{k = 0}^\infty \gamma^k R_{t + k + 1} \bigg| S_t =s \right], \text{ for all } s \in S$$

给定策略 $\pi$，行为估值函数(Action Value Function)定义为：

$$q_\pi(s, a) \doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi \left[ \sum_{k = 0}^\infty \gamma^k R_{t + k + 1} \bigg | S_t = s, A_t = a\right]$$

估值函数的贝尔曼方程：

$$\begin{aligned}
v_\pi(s) \doteq & \mathbb{E}_\pi[G_t | S_t =s ] \\
=& \mathbb{E}_\pi[R_{t + 1} + \gamma G_{t + 1} | S_t =s ] \\
=& \sum_a \pi(a | s) \sum_{s'} \sum_r p(s', r | s, a) \left[r + \gamma \mathbb{E}_\pi [G_{t + 1} | S_{t + 1} = s'] \right] \\
=& \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a) \left[r + \gamma  v_\pi(s') \right], \text{ for all } s \in S \\
\end{aligned}$$


贝尔曼方程定义了状态估值函数的依赖关系：

- 给定策略下，每个状态的估值视为一个变量
- 所有状态（假如有 $n$ 个）的估根据贝尔曼方程形成了 **一个具有 $n$ 个方程和 $n$ 个变量的线性方程组**
- 求解该方程组即可得到该策略下每个状态的估值

----

状态估值函数定义了策略空间上的一个偏序，给定两个策略 $\pi$ 和 $\pi'$，如果对于所有状态 $s$，都有 $v_\pi(s) \geqslant v_{\pi'}(s)$，则我们说 $\pi \geqslant \pi'$

最优策略 $\pi_*$ 对应于如下状态估值函数：

$$v_*(s) = \max_{\pi} v_\pi(s)$$

最优策略可以有多个，对应的状态估值函数都一样

最优策略对应的行为估值函数：

$$q_*(s, a) = \max_\pi q_\pi(s, a)$$

状态估值函数的贝尔曼最优性方程：

$$\begin{aligned}
v_*(s) =& \max_{a \in A(s)} q_{\pi_*}(s, a) \\
=& \max_{a} \mathbb{E}_{\pi_*}[G_t | S_t = s, A_t = a] \\
=& \max_{a} \mathbb{E}_{\pi_*}[R_t + \gamma G_{t + 1} | S_t = s, A_t = a] \\
=& \max_{a} \mathbb{E}[R_t + \gamma v_*(S_{t + 1}) | S_t = s, A_t = a] \\
=& \max_{a} \sum_{s', r} p(s', r| s, a)[r + \gamma v_*(s')] \\
\end{aligned}$$

行为估值函数的贝尔曼最优方程：

$$\begin{aligned}
q_*(s, a) =& \mathbb{E} [R_{t + 1} | \gamma \max_{a'} q_*(S_{t + 1} , a')| S_t = s, A_t = a] \\
=& \sum_{s', r}p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] \\
\end{aligned}$$

基于状态估值函数的贝尔曼最优性方程

- 第一步：求解状态估值函数的贝尔曼最优性方程得到最优策略对应的状态估值函数
- 第二步：根据状态估值函数的贝尔曼最优性方程，进行一步搜索找到每个状态下的最优行为
    - 注意：最优策略可以存在多个
    - 贝尔曼最优性方程的优势，可以采用贪心局部搜索即可得到全局最优解

基于行为估值函数的贝尔曼最优性方程
- 直接得到最优策略

求解贝尔曼最优性方程寻找最优策略的局限性

- 需要知道环境模型
- 需要高昂的计算代价和内存（存放估值函数）
- 依赖于马尔科夫性

----

**贝尔曼最优性方程**

给定策略 $\pi$，状态估值函数的贝尔曼方程：

$$v_\pi(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r| s, a)[r + \gamma v_\pi(s')], \text{ for all } s \in S$$

状态估值函数的贝尔曼最优性方程

$$\begin{aligned}
v_*(s) =& \max_a \mathbb{E}[R_{t + 1} + \gamma v_*(S_{t + 1}) | S_t = s, A_t = a] \\
=& \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_*(s')]
\end{aligned}$$

### 动态规划方法

给定策略 $\pi$，的状态估值函数的更新规则：

$$v_\pi(s) \Leftarrow \sum_a \pi(a|s) \sum_{s', r} p(s', r| s, a)[r + \gamma v_\pi(s')], \text{ for all } s \in S$$

最优状态估值函数的更新规则

$$\begin{aligned}
v_*(s) \Leftarrow& \max_a \mathbb{E}[R_{t + 1} + \gamma v_*(S_{t + 1}) | S_t = s, A_t = a] \\
=& \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_*(s')]
\end{aligned}$$

其中 $\Leftarrow$ 表示赋值操作，而不是等于符号。

----

给定策略 $\pi$，该策略下的状态估值函数满足：

$$\begin{aligned}
v_\pi(s) \doteq& \mathbb{E}_\pi [G_t | S_t = s] \\
=& \mathbb{E}_\pi [R_{t + 1} + \gamma G_{t + 1} | S_t = s] \\
=& \mathbb{E}_\pi [R_{t + 1} + \gamma v_\pi(S_{t + 1}) | S_t = s] \\
=& \sum_{a} \pi(a | s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \\
\end{aligned}$$

假如环境 $p(s', r | s, a)$ 已知，状态估值函数的求解方式有：

- 求解线性方程组，但是计算开销大
- 寻找不动点，迭代策略估值

---

迭代策略估值的更新规则

$$\begin{aligned}
v_{k + 1}(s) \doteq& \mathbb{E}_\pi[R_{t + 1} + \gamma v_k(S_{t + 1}) | S_t = s] \\
=& \sum_{a} \pi(a | s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')]
\end{aligned}$$

又被称为 **期望更新**

> 状态 $s$ 新一轮的估值是基于 $s$ 所有可能的下一状态 $s'$ 的期望计算得到的（注意：不是基于某一次采样）

----

迭代策略估值的实现，两种实现方式：

- 同步更新：两个数组存放 新 和 旧 的状态估值
- 异步跟新：收敛速度快一些，尽早利用了新信息，使用一个数据组，原地更新；

---

策略估值的目标是为了寻找更优的策略（策略提升）

- 策略估值根据策略 $\pi$ 计算其估值函数 $v_\pi$

策略提升：

- 根据当前策略的估值函数，寻找更优的策略（如果存在），逐步寻找到最优策略，根据策略 $\pi$ 的估值函数 $v$ 寻找更优策略 $\pi'$

提升方法

- 给定一个确定策略 $\pi$，在状态 $s$ 下选择行为 $a$，后续按照策略 $\pi$ 行动所得的估值 $q_\pi(s, a)$ 是否高于完全按照策略 $\pi$ 行动得到的估值 $v_\pi(s)$

$$\begin{aligned}
q_\pi(s, a) \doteq& \mathbb{E} [R_{t + 1}, \gamma v_\pi(S_{t + 1}) | S_t = s, A_t = a] \\
=& \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
\end{aligned}$$

---

策略提升定理

对于两个确定式策略 $\pi$ 和 $\pi'$，如果对于所有状态 $s$ 均满足

$$q_\pi(s, \pi'(s)) \geqslant v_\pi(s)$$

则策略 $\pi'$ 优于策略 $\pi$，即

$$v_{\pi'}(s) \geqslant v_\pi(s)$$

> 注意：策略提升方法是策略提升定理的一个特例

给定策略 $\pi$，按照贪心方式得到更优策略 $\pi'$

$$\begin{aligned}
\pi'(s) \doteq& \underset{a}{\text{arg max }} q_\pi(s, a) \\
=& \underset{a}{\text{arg max }} \mathbb{E} [R_{t + 1} + \gamma v_\pi(S_{t + 1}) | S_t=s, A_t=a] \\
=& \underset{a}{\text{arg max }} \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]\\
\end{aligned}$$

从初始策略 $\pi_0$ 开始，迭代进行 “策略估值(E)” 和 “策略提升(I)”，最终得到最优策略 $\pi_*$

$$\pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} v_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \cdots \xrightarrow{I} \pi_* \xrightarrow{E} v_{\pi_*}$$

----

策略迭代分析：

策略迭代过程中，策略估值需要多次扫描更新状态估值，精确估计出当前策略的状态估值，耗费了大量计算时间

- 是否可以在不精确的状态估值下，进行策略提升呢？譬如：状态估值进行一轮扫描更新后，便进行策略提升

$$\begin{aligned}
v_{k + 1}(s) \doteq& \max_a \mathbb{E} [R_{t + 1} + \gamma v_k(S_{t + 1}) | S_t = s, A_t = a] \\
=& \max_a \sum_{s',r} p(s', r|s, a)[r + \gamma v_k(s')] \\
\end{aligned}$$

不显式给出当前的策略，而是直接根据当前估值按照 **贪心策略** 估计下一轮的值；

估值迭代：从初始状态估值 $v_0$ 开始，进行估值迭代，找到最优状态估值 $v_*$，进而根据 $v_*$ 按照贪心方式得到最优策略 $\pi_*$

todo:code..

---

迭代进行两个阶段：

- 策略估值：让新的估值和当前的策略保持一致
- 策略提升：根据当前估值，得到相应的贪心策略

动态规划方法小结

- 动态规划方法只不过是把贝尔曼方程转变为更新规则
    - 四个贝尔曼方程对应着四个更新规则 $(v_\pi, v_*, q_\pi, q_*)$
- 动态规划方法是一种“自举”(bootstrapping)方法
- 优势：动态规划方法计算效率高
- 缺点：动态规划方法需要知道关于环境的完整模型

---

### 蒙特卡洛方法

- 从真实或者模拟的经验(experience)中计算状态（行动）估值函数
- 不需要关于环境的完整模型

状态估值：从某个状态 $s$ 出发，使用当前策略 $\pi$ 通过蒙特卡洛模拟的方式生成多个 episode，使用这些episode 的平均收益 (return) 近似状态估值函数 $v_\pi(s)$

todo:code...

收敛速度大约为： $1 \over \sqrt{n}$（$n$ 表示蒙特卡洛模拟次数）

---

- 在完整的环境模型未知时，仅有状态估值 $v_\pi(s)$ 无法得出策略 $\pi$
- 大多数情况下，直接使用蒙特卡洛方法计算行为估值函数 $q_\pi(s, a)$，进而采用贪心方法得到策略 $\pi$


部分“状态-行为”在蒙特卡洛模拟中可能不出现

解决方法

- Exploring Start：每个“状态-行为”对都以一定的概率作为蒙特卡洛模拟的起始点

两种方法：

On-policy方法：

- 在每个状态 $s$ 下保持对所有行为 $A(s)$ 进行探索的可能性，譬如采用 $\varepsilon$ 贪心策略，以 $\displaystyle 1 - \varepsilon + {\varepsilon \over |A(s)|}$ 选择贪心行为，以 $\displaystyle {\varepsilon \over |A(s)|}$的概率选择任意非贪心行为
- 缺点：最终得到的最优策略仅仅是 $\varepsilon$ 最优策略（$\varepsilon$-soft policy）
- 放弃了最优性来换取对策略的探索

Off-policy方法
- 使用两个策略：目标策略 $\pi$ 和行为策略 $b$
- 目标策略是待优化的策略，以贪心方式进行
- 行为策略保证在每个状态下对所有行为进行探索的可能性

保证寻找最优策略，在优化目标策略 $\pi$ 时，用行为策略 $b$ 进行策略探索

- 行为策略需要确保对行为的覆盖度（coverage），对于所有 $\pi(a | s) > 0$，需要有 $b(a | s) > 0$
- 缺点：方差比较大，收敛慢
- 行为策略的选择影响收敛速度和方差，通过重要度采样方式对蒙特卡洛模拟结果进行加权

---

### 时序差分方法

非平稳情形下的蒙特卡洛方法（恒定步长）

$$v(S_t) \leftarrow v(S_t) + \alpha [G_t - v(S_t)]$$

其中：

- $G_t$ 表示第 $t$ 轮蒙特卡洛模拟的收益
- $v(S_t)$ 表示第 $t$ 轮对状态 $S_t$ 的估值

时序差分方法（Temporal Difference: TD）

$$v(S_t) \leftarrow v(S_t) + \alpha [R_{t + 1} + \gamma v(S_{t + 1}) - v(S_t)]$$

不需要根据完整的 episode 计算 $G_t$，只需要模拟几步（这里是 1 步）之后更新状态估值；

---

- 一种在线的从经验中进行策略学习的方法
- 一般直接学习行为估值函数完成策略学习
- 适用于状态和行为空间比较小的问题

---

### 参数化近似方法

当状态空间或行为空间比较大时，采用表格方式存放状态估值或行为估值不可行，需要对状态估值或行为估值进行参数化近似(parametrized approximation)

参数化的函数形式

- 广义线性模型，参数是特征的权重
- 决策树，参数是叶子节点的取值，和树节点分裂的阈值
- 神经网络，每层的连接权重

一般要求：参数个数要小于状态（或状态-行为）的个数

---

参数化近似方法的参数学习

训练样本形式

- 动态规划方法

$$s \mapsto \mathbb{E}_\pi [R_{t + 1} + \gamma \hat{v}(S_{t + 1}, w_t) | S_t = s]$$

- 蒙特卡洛方法

$$S_t \mapsto G_t$$

- 时序差分方法

$$S_t \mapsto R_{t + 1} + \gamma \hat{v} (S_{t + 1}, w_t)$$

哪些监督学习方法适合于强化学习的参数学习呢？
- 能够进行在线训练，应对目标函数的不平稳或训练样本标注的不平稳
- 不能依赖于对训练样本的多次扫描

---

## 博弈

### 纳什均衡

### 机制设计

### 拍卖

### 讨价

### 两种策略

### 匹配市场

### 中介市场

### 议价权

## 统计中的因果推断

### 为什么学习因果推断？

克服传统方法的不足

- 相关不意味着因果
- 缺少从数据中解读因果关系的有效数学语言或工具
- 统计无法回答反事实问题

### 辛普森悖论

**总体数据** 上得出的统计结论和 **分组数据** 上的统计结论相反

## 参考文献

1. <t id='mp'/> McClloch and Pitts, A logical calculus of the ideas immanent in nervous activity, 1943
2. <t id='infer'/>Judea Pearl, Madelyn Glymour, Nicholas P. Jewell Causal Inference in Statistics, Wiley 2016
3. <t id='resolution'/> J. A. Robinson. A machine-oriented logic based on the resolution principle.  Journal of the ACM, 1965, 12(1):23-41
4. <t id='ant'/>  A. Colorni, M. Dorigo et V. Maniezzo, Distributed Optimization by Ant Colonies, actes de la première conférence européenne sur la vie artificielle, Paris, France, Elsevier Publishing, 134-142, 1991.
5. <t id='particle'/> James Kennedy and Russell Eberhart. Particle swarm optimization. Proceedings of the IEEE International Conference on Neural Networks, pp. 1942–1948, Piscataway, NJ, 1995. 


