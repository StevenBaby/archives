# 一些笔记

[annotation]: [id] (2ce7a71d-a8c3-4fa7-b25f-50392d96e956)
[annotation]: [status] (protect)
[annotation]: [create_time] (2021-12-12 20:25:44)
[annotation]: [category] (计算机科学)
[annotation]: [tags] (人工智能|机器学习)
[annotation]: [comments] (false)
[annotation]: [url] (http://blog.ccyg.studio/article/2ce7a71d-a8c3-4fa7-b25f-50392d96e956)

## Dropout 

随机的将隐藏层输出的一些权值设为 0

## 优化算法

其中 $x$ 为向量

$$f(x + \delta x) \approx f(x) + \nabla f(x) \cdot \delta x + \delta x \cdot H \cdot \delta x$$

$$f(x + \delta x) \approx f(x) + \nabla f(x) \cdot \delta x$$

$$f(x + \delta x)  - f(x) \approx \nabla f(x) \cdot \delta x \leqslant 0$$


当 $\delta x = -\eta \nabla f$ 时，$\nabla f(x) \cdot \delta x =  -\eta \nabla f^2 \leqslant 0$


### 梯度下降法

如果相求一个函数的极小值，使得每次迭代均减去负梯度即可。

$$f(x_1, x_2) = x_1^2 + 2 x_2^2 + x_1$$


$$W_t = W_{t - 1} - \eta \cdot \Delta W$$

---

动量法，指数加权移动平均法，其中 $\beta$ 为动量因子

$$v_t =\beta v_{t - 1} + (1 - \beta) g_w$$

$$\hat{v}_t = {v_t \over 1 - \beta^t}$$

$$w_t = w_{t - 1} - \eta \cdot \hat{v}_t$$

---

Nesterov 算法

$$\Delta W_t = {\partial J(W_{t - 1}  + \gamma V_{t - 1}) \over \partial W}$$

AdaGrad

$$W_t = W_{t - 1} - {\eta \over \sqrt{S_t} + \varepsilon} \cdot \Delta W_t$$

$$S_t = S_{t - 1} + \Delta W_t \cdot \Delta W_t$$

$$\Delta W_t = {\partial J(W_{t - 1}) \over \partial W}$$

Adam

## 反向传播

- 随机梯度下降 Stochastic Gradient Descent




