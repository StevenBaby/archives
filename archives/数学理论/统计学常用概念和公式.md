# 统计学常用概念和公式

[annotation]: <id> (6bc4e989-0a43-4201-9a5b-9cc5abf8d926)
[annotation]: <status> (public)
[annotation]: <create_time> (2019-10-12 14:33:39)
[annotation]: <category> (数学理论)
[annotation]: <tags> (概率论|统计学)
[annotation]: <comments> (false)
[annotation]: <url> (http://blog.ccyg.studio/article/6bc4e989-0a43-4201-9a5b-9cc5abf8d926)

<input class='mathjax align' value='left' type='hidden'/>

## 基本概率公式

- 逆概率公式

$$
P(\overline{A}) = 1 - P(A)
$$

- 加法公式

$$
P(A + B) = P(A) + P(B) - P(AB)
$$

- 减法公式

$$
P(A - B) = P(A) - P(AB) = P(A\overline{B})
$$

- 条件概率

$$
P(B|A) = \frac{P(AB)}{P(A)}
$$

- 乘法公式

$$
\begin{aligned}
P(AB) &= P(A)\cdot P(B|A) \\
P(AB) &= P(B)\cdot P(A|B) \\
\end{aligned}
$$

- 全概率公式

$$
P(B) = \sum_{i=1}^{n}P(A_i)\cdot P(B | A_i)
$$

- 贝叶斯公式

$$
P(A_j|B) = \frac{P(A_jB)}{P(B)} = \frac{P(A_j)P(B|A_j)}{\sum\limits_{i=1}^{n}P(A_i)\cdot P(B | A_i)}
$$

- 相互独立

$$
P(AB) = P(A) \cdot P(B)
$$

> 如果随机变量 $X, Y$ 相互独立，那么 $g(X), h(Y)$ 也相互独立。

---

## 一维随机变量的分布

### 分布函数的应用

$F(a - 0)$ 是 $F(x)$ 在 $a$ 处的左极限。

$$
\begin{aligned}
P\{X \leqslant a\} &= F(a) \\
P\{X < a\} &= F(a - 0)\\
P\{X = a\} &= F(a) - F(a - 0) \\
\end{aligned}
$$

$$
\begin{aligned}
F(x) &= P\{X\leqslant x\} = \sum_{x_i \leqslant x}P\{X = x_i\} \\
F(x) &= P\{X\leqslant x\} = \int_{-\infty}^x f(t)dt \ \ (x\in R)
\end{aligned} 
$$

$$
P\{a < X < b\} = P\{a \leqslant X < b\} = P\{a < X \leqslant b\} = P\{a \leqslant X \leqslant b\} = \int_{a}^b f(t)dt = F(b) - F(a)
$$


### 伯努利分布

0-1 分布，（比如掷硬币，射箭中与不中）

$$
\begin{aligned}
X &\sim B(1, p)\\
P\{X=k\} &= p^k(1-p)^{1-k}, (k=0,1)\\
EX &= p\\
DX &= p(1-p)
\end{aligned}
$$

### 二项分布

多次同分布试验（比如多次掷骰子，掷硬币）

$$
\begin{aligned}
X &\sim B(n, p)\\
P\{X=k\} &= C_n^k p^k(1-p)^{n-k}, (k=0,1,\cdots,n)\\
EX &= np\\
DX &= np(1-p)
\end{aligned}
$$

### 泊松分布

质点流量（比如一段时间内买东西的顾客数量 $k$ 的概率）

$$
\begin{aligned}
X &\sim P(\lambda)\\
P\{X=k\} &= \frac{\lambda^k}{k!}e^{-\lambda}\\
EX &= \lambda\\
DX &= \lambda
\end{aligned}
$$

### 几何分布

首中即停止，与**几何**无关，比如一直投篮知道投中为止，投篮次数 $k$ 的概率

$$
\begin{aligned}
X &\sim G(p)\\
P\{X=k\} &= (1-p)^{k-1}p\\
EX &= \frac{1}{p}\\
DX &= \frac{1 - p}{p^2}
\end{aligned}
$$

### 超几何分布

总共有 $N$ 个球，其中有 $M$ 个是红色的，是从中不放回地取 $n$ 个球，其中有 $k$ 个是红球的概率；

$$
\begin{aligned}
X &\sim H(n, N, M)\\
P\{X=k\} &= \frac{C_M^kC_{N-M}^{n-k}}{C_N^n} ,\ (k \leqslant min\{M, n\})\\
EX &= n\frac{M}{N}\\
DX &= n\frac{M}{N}\cdot(1-\frac{M}{N})\cdot\frac{N-n}{N-1}
\end{aligned}
$$

### 均匀分布

$$
\begin{aligned}
X &\sim U(a, b)\\
f(x) &= \left\{
\begin{aligned}
&\frac{1}{b-a}, a < x < b\\
&0, x = other
\end{aligned}
\right.\\
F(x) &= \left\{
\begin{aligned}
&0, x < a\\
&\frac{x - a}{b-a}, a \leqslant x < b\\
&1, x \geqslant 0
\end{aligned}
\right.\\
EX &= \frac{b + a}{2}\\
DX &= \frac{(b - a)^2}{12}
\end{aligned}
$$


### 指数分布

质点间隔时间（与泊松分布相对，比如买东西的两个顾客之间连续的时间间隔）

$$
\begin{aligned}
X &\sim E(\lambda)\\
f(x) &= \left\{
\begin{aligned}
&\lambda e^{-\lambda x}, x > 0\\
&0, x \leqslant 0
\end{aligned}
\right.\\
F(x) &= \left\{
\begin{aligned}
&1 - e^{-\lambda x}, x > 0 \\
&0, x \leqslant 0\\
&(\lambda > 0) \\
\end{aligned}
\right.\\
EX &= \frac{1}{\lambda}\\
DX &= \frac{1}{\lambda^2}
\end{aligned}
$$

### 正态分布

世间万物的终极法则，中心极限定理的归宿。

$$
\begin{aligned}
X &\sim N(\mu, \sigma^2)\\
f(x) &= \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2}}\\
EX &= \mu\\
DX &= \sigma^2 \\
F(x) &= P\{X \leqslant x \} = \Phi(\frac{x - \mu}{\sigma})\\
1 &= F(\mu - x) + F(\mu + x)\\
P\{a \leqslant X \leqslant b \} &= \Phi(\frac{b - \mu}{\sigma}) - \Phi(\frac{a - \mu}{\sigma})\\
aX + b &\sim N(a\mu + b, a^2\sigma^2)
\end{aligned}
$$

### 标准正态分布

$$
\begin{aligned}
X &\sim N(0, 1)\\
f(x) &= \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}\\
\Phi(0) &= \frac{1}{2} \\
\Phi(-x) &= 1 - \Phi(x)\\
EX &= 0\\
DX &= 1 \\
\end{aligned}
$$

## 多维随机变量的分布

### 联合分布函数

$$
F(x, y) = 
P\{X \leqslant x, Y \leqslant y\} = 
\sum_{x_i \leqslant x}
\sum_{y_j \leqslant y} p_{ij}
$$

### 边缘分布

$$
\begin{aligned}
p_{i.} &= P\{X = X_i\} = 
\sum_{j=1}^n P\{X = x_i, Y = y_j\} =
\sum_{j=i}^\infty p_{ij} \ (i=1,2,\cdots)\\
p_{.j} &= P\{Y = Y_j\} = 
\sum_{i=1}^n P\{X = x_i, Y = y_j\} =
\sum_{i=i}^\infty p_{ij} \ (i=1,2,\cdots)\\
\end{aligned}
$$


### 概率分布函数与概率密度

$$
F(x, y) = \int_{-\infty}^x\int_{-\infty}^yf(u, v)dudv
$$

$$
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x, y)dxdy = 1
$$

$$
\frac{\partial^2F(x, y)}{\partial x \partial y} = f(x, y)
$$

### 边缘分布函数

$$
F_x(x) = F(x, +\infty) = 
\int_{-\infty}^x[
    \int_{-\infty}^{+\infty}
    f(u, v)dv
]du
$$

### 边缘概率密度

如果 $X$ 是连续随机变量，其概率密度：

$$
f_X(x) =
\int_{-\infty}^{+\infty}f(x, y)dy
$$

### 条件概率密度

设 $(X,Y) \sim f(x, y)$ 边缘概率密度 $f_X(x) > 0$

$$
f_{Y|X}(y|x) = \frac{f(x, y)}{f_X(x)}
$$

### 二维均匀分布

$$
f(x, y) = \left\{
\begin{aligned}
&\frac{1}{S_D},\ (x, y)\in D\\
&0,\ other
\end{aligned}
\right.\\
$$

### 二维正态分布

$(X, Y) \sim N(\mu_1, \mu_2; \sigma_1^2, \sigma_2^2; \rho)$

$$
f(x, y) = \frac{1}
{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}
\exp\{
    -\frac{1}{2(1-\rho^2)}[
        (\frac{x-\mu_1}{\sigma_1})^2 - 
        2\rho(\frac{x-\mu_1}{\sigma_1})(\frac{y-\mu_2}{\sigma_2}) + (\frac{y - \mu_2}{\sigma_2})^2
]\}
$$

---

### 随机变量函数的分布 (卷积公式)

1. $X + Y$分布

$$
\begin{aligned}
f_Z(z) = &\int_{-\infty}^{+\infty} f(x, z-x) dx = \int_{-\infty}^{+\infty} f(z-y, y) dy \\
\xlongequal{独立} &\int_{-\infty}^{+\infty} f_X(x)f_Y(z-x) dx = \int_{-\infty}^{+\infty} f_X(z-y)f_Y(y) dy
\end{aligned}
$$

2. $X - Y$分布

$$
\begin{aligned}
f_1(z) = &\int_{-\infty}^{+\infty} f(x, x-z) dx = \int_{-\infty}^{+\infty} f(y + z, y) dy \\
\xlongequal{独立} & \int_{-\infty}^{+\infty} f_X(y + z)f_Y(y) dy
\end{aligned}
$$

3. $XY$ 分布

$$
\begin{aligned}
f_Z(z) = &\int_{-\infty}^{+\infty} 
\frac{1}{|x|}f(x, \frac{z}{x}) dx 
= \int_{-\infty}^{+\infty} 
\frac{1}{|y|}f(\frac{z}{y}, y) dy \\
\xlongequal{独立} &\int_{-\infty}^{+\infty} 
\frac{1}{|x|}f_X(x)f_Y(\frac{z}{x}) dx 
= \int_{-\infty}^{+\infty} 
\frac{1}{|y|}f_X(\frac{z}{y})f_Y(y) dy
\end{aligned}
$$

4. $\frac{X}{Y}$分布

$$
\begin{aligned}
f_Z(z) = &\int_{-\infty}^{+\infty}
|y| f(yz, y) dy \\
\xlongequal{独立} &\int_{-\infty}^{+\infty}
|y|f_X(yz)f_Y(y) dy
\end{aligned}
$$


5. $\max\{ X, Y\}$ 分布

$$
\begin{aligned}
F_{\max}(z) = &P\{\max\{X,Y\} \leqslant z\} \\
= &P\{X \leqslant z, Y \leqslant z\} \\
= &F(z, z) \\
\xlongequal{独立} &F_X(z)F_Y(z)
\end{aligned}
$$

6. $\min\{ X, Y\}$ 分布

$$
\begin{aligned}
F_{\min}(z) = &P\{\min\{X,Y\} \leqslant z\} \\
= &P\{\{X \leqslant z\} \cup \{Y \leqslant z \}\} \\
= &P\{X \leqslant z\} + P\{Y \leqslant z \} - P\{X \leqslant z, Y \leqslant z\} \\
= &F_X(z) + F_Y(z) - F(z, z) \\
\xlongequal{独立} &F_X(z) + F_Y(z) - F_X(z)F_Y(z) \\
= & 1 - [1-F_X(z)][1-F_Y(z)]
\end{aligned}
$$

### 常见分布的可加性

$$
X \sim B(n,p), Y \sim B(m, p) 
\Rightarrow 
X + Y \sim B(n + m, p)
$$

$$
X \sim P(\lambda_1), Y \sim P(\lambda_2)
\Rightarrow 
X + Y \sim P(\lambda_1 + \lambda_2)
$$

$$
X \sim N(\mu_1,\sigma_1^2), Y \sim N(\mu_2,\sigma_2^2) 
\Rightarrow 
X + Y \sim N(\mu_1 + \mu_2,\sigma_1^2 + \sigma_2^2)
$$

$$
X \sim χ^2(n), Y \sim χ^2(m)
\Rightarrow 
X + Y \sim χ^2(n + m)
$$

---

## 随机变量数字特征

$$
\begin{aligned}
EX &= \sum_{i=1}^{\infty} x_ip_i \\
EX &= \int_{-\infty}^{+\infty} xf(x) dx
\end{aligned}
$$

---

$$
\begin{aligned}
E(\sum_{i=1}^{n} a_iX_i) &= \sum_{i=1}^{n} a_i EX_i \\
Ec &= c \\
E(aX + c) &= aEX + c \\
E(X \pm Y) &= EX \pm EY \\
E(XY) &\xlongequal{独立} EXEY \\
\end{aligned}
$$

---

$$
\begin{aligned}
DX &= Var(X) = E(X - EX)^2 = E(X^2) - (EX)^2\\
D(aX + b) &= a^2DX\\
D(X \pm Y) &= DX + DY \pm 2{\rm Cov}(X, Y)\\
E(X^2) &= DX + (EX)^2 \\
X^* &= \frac{X - EX}{\sqrt{DX}} \\
DX &= E(X - EX)^2 \leqslant E(X - c)^2
\end{aligned}
$$

---

如果 $X, Y$ 独立：

$$
\begin{aligned}
D(aX + bY) &= a^2DX + b^2DY \\
D(XY) &= DX\cdot DY + DX(EY)^2 + DY(EX)^2 \geqslant DX\cdot DY 
\end{aligned}
$$

### 多维随机变量的数字特征

$$
\begin{aligned}
{\rm Cov}(X, Y) &= E((X - EX)(Y - EY)) = E(XY) - EX EY\\
\rho_{XY} &= \frac{{\rm Cov}(X, Y)}{\sqrt{DX}\sqrt{DY}} \\
\end{aligned}
$$

---

$$
E(XY) = \left\{
\begin{aligned}
&\sum_i\sum_j x_iy_jP\{X=x_i, Y=y_j\} \ 离散型\\
&\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} xyf(x,y) dxdy \ 连续型
\end{aligned}
\right.
$$

---

**对称性**

$$
\begin{aligned}
{\rm Cov}(X, Y) &= {\rm Cov}(Y, X) \\
\rho_{XY} &= \rho_{YX} \\
{\rm Cov}(X, X) &= DX \\
\rho_{XX} &= 1 \\
\end{aligned}
$$

**线性性**

$$
\begin{aligned}
{\rm Cov}(X, c) &= 0 \\
{\rm Cov}(aX + b, Y) &= a{\rm Cov}(X, Y) \\
{\rm Cov}(X_1 + X_2, Y) &= {\rm Cov}(X_1, Y) + {\rm Cov}(X_2, Y) \\
\end{aligned}
$$

**相关系数有界性**

$$|\rho_{XY}| \leqslant 1$$

---

如果 $Y = aX + b$ 则：

$$
\rho_{XY} = \left\{
\begin{aligned}
&1, a > 0 \\
&-1, a < 0 \\
\end{aligned}
\right.
$$

---

## 大数定律与中心极限定理

---

### 切比雪夫不等式

如果随机变量 $X$ 的方差 $DX$ 存在，则对任意 $\varepsilon > 0$ 有：

$$
\begin{aligned}
P\{|X - EX| \geqslant \varepsilon \} &\leqslant \frac{DX}{\varepsilon^2} \\
P\{|X - EX| < \varepsilon \} &\geqslant 1 - \frac{DX}{\varepsilon^2} \\   
\end{aligned}
$$

---

### 切比雪夫大数定律

假设 $\{X_n\}(n = 1,2,\cdots)$ 是相互独立的随机变量序列，如果方差 $DX_k(k\geqslant1)$ 存在且一致有上界，即存在常数 $C$ 使 $DX_k \leqslant C$ 对一切 $k\geqslant1$ 均成立，则 $\{X_n\}$ 服从大数定律：

$$
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} 
\frac{1}{n}\sum_{i=1}^{n} EX_i 
$$

---

### 伯努利大数定律

假设 $\mu_n$ 是 $n$ 重伯努利试验中事件 $A$ 发生的次数，在每次试验中事件 $A$ 发生的概率为 $p(0 < p < 1)$，则 $\frac{\mu_n}{n} \xrightarrow{P} p$，即对任意 $\varepsilon > 0$ 有：

$$
\lim_{n\to\infty} P\{ |\frac{\mu_n}{n} - p | < \varepsilon \} = 1
$$

---

### 辛钦大数定律

假设 $\{X_n\}$ 是独立同分布的随机变量序列，如果 $EX_n = \mu$ 存在，则 $\frac{1}{n}\sum\limits_{i=i}^n X_i \xrightarrow{P} \mu$，即对任意 $\varepsilon > 0$ 有：

$$
\lim_{n\to\infty} P\{ |\frac{1}{n}\sum_{i=1}^n X_i - \mu|
 < \varepsilon \} = 1
$$

---

### 列维-林德伯格定理

假设 $\{X_n\}$ 是独立同分布的随机变量序列，如果 $EX_n = \mu, DX_n=\sigma^2 > 0(n \geqslant 0)$ 存在，则 $\{X_n\}$ 服从中心极限定理，即对任意的实数 $x$ 有：

$$
\lim_{n\to\infty}P\left\{
    \frac{\sum\limits_{i=1}^n X_i - n\mu}{\sqrt{n}\sigma} 
    \leqslant x
\right\} =
\frac{1}{\sqrt{2\pi}}
\int_{-\infty}^x e^{-\frac{t^2}{2}}dt = \Phi(x)
$$

---

### 棣莫弗-拉普拉斯定理

假设随机变量 $Y_n \sim B(n,p)(0<p<1, n \geqslant 1)$，则对任意实数 $x$，有：

$$
\lim_{n\to\infty}\{
\frac{Y_n - np}{\sqrt{np(1-p)}}
\leqslant x
\} = 
\frac{1}{\sqrt{2\pi}}
\int_{-\infty}^x e^{-\frac{t^2}{2}}dt = \Phi(x)
$$

---

## 数理统计与分布

### 样本数字特征

**样本均值**

$$
\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
$$

**样本方差**

$$
S^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \overline{X})^2 = \frac{1}{n-1}(\sum_{i=1}^nX_i^2 - n\overline{X}^2)
$$

**样本标准差**

$$
S = \sqrt{\frac{1}{n-1} \sum_{i=1}^n(X_i - \overline{X})^2}
$$

**样本 $k$ 阶原点矩**

$$
A_k = \frac{1}{n} \sum_{i=1}^n X_i^k \ (k=1,2,\cdots)
$$

**样本 $k$ 阶中心矩**

$$
B_k = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^k \ (k=1,2,\cdots)
$$

### 顺序统计量（次序统计量）

将样本 $X_1, X_2, \cdots, X_n$ 的 $n$ 个观测量按其取值从大到小的顺序排列得：

$$
X_{(1)} \leqslant X_{(2)} \leqslant \cdots \leqslant X_{(n)}
$$

随机变量 $X_{(k)}(k = 1,2,\cdots, n)$ 称作 **第k顺序统计量**， 其中 $X_{(1)}$ 是最小观测量，而 $X_{(n)}$ 是最大观测量。

$$
\begin{aligned}
X_{(1)} &= \min\{ X_1, X_2, \cdots, X_n\} \to 
F_{(1)}(x) = 1 - [1 - F(x)]^n  \\
X_{(n)} &= \max\{ X_1, X_2, \cdots, X_n\} \to
F_{(n)}(x) = [F(x)]^n  \\
\end{aligned}
$$

### 常用统计量的性质

$$
\begin{aligned}
EX_i &= \mu \\
DX_i &= \sigma^2 \\
E\overline{X} &= EX = \mu \\
D\overline{X} &= \frac{1}{n}DX = \frac{\sigma^2}{n} \\
E(S^2) &= DX = \sigma^2
\end{aligned}
$$

### 卡方分布

若随机变量 $X_1, X_2, \cdots, X_n$ 相互独立，且都服从标准正态分布，则随机变量

$$X = \sum\limits_{i=1}^n X_i^2 $$ 

服从自由度为 $n$ 的 $χ^2$ 分布，记为$X \sim χ^2(n)$，特别地，$X_i \sim χ^2(1)$。

对给定的 $\alpha(0<\alpha<1)$ 称满足：

$$
P\{χ^2 > χ_\alpha^2(n)\} =
\int_{χ_\alpha^2(n)}^{+\infty} f(x) dx = \alpha
$$

的 $χ_\alpha^2(n)$ 为 $χ^2(n)$ 分布的 **上α分位点**。

---

若 $X_1 \sim χ^2(n_1), X_2 \sim χ^2(n_2)$，$X_1$ 与 $X_2$ 相互独立，则 $X_1 + X_2 \sim χ^2(n_1 + n_2)$

若 $X \sim χ^2(n)$，则 $EX = n, DX=2n$

### t 分布

设随机变量 $X\sim N(0, 1), Y\simχ^2(n)$，$X$ 与 $Y$ 相互独立，则随机变量：

$$
t = \frac{X}{\sqrt{Y/n}}
$$

服从自由度为 $n$ 的 t分布，记为 $t \sim t(n)$。

**t分布的性质**

$$
P\{t > -t_{\alpha}(n)\} = P\{t > t_{1-\alpha}(n)\}
$$

### F 分布

设随机变量 $X\sim χ^2(n_1), Y\simχ^2(n_2)$，$X$ 与 $Y$ 相互独立，则随机变量：

$$
f = \frac{X/n_1}{Y/n_2}
$$

服从自由度为 $n_1, n_2$ 的 F分布，记为 $F \sim F(n_1, n_2)$。

**F分布的性质**

$$
X \sim F(n_1, n_2) \Rightarrow \frac{1}{X} \sim F(n_2, n_1)
$$

$$
F_{1-\alpha}(n_1, n_2) = \frac{1}{F_{\alpha}(n_2, n_1)}
$$

### 正态总体条件

设 $X_1, X_2, \cdots, X_n$ 是取自正态总体 $N(\mu, \sigma^2)$ 的一个样本， $\overline{X}, S^2$ 分别是样本的均值和方差，则：

$$
\begin{aligned}
\overline{X} &\sim N(\mu, \frac{\sigma^2}{n}) \\
\end{aligned}
$$

$$
\frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\sqrt{n}(\overline{X} - \mu)}{\sigma} \sim N(0, 1)
$$

$$
\frac{1}{\sigma^2}\sum_{i=1}^n(X_i - \mu)^2 \sim χ^2(n)
$$

$$
\frac{(n-1)S^2}{\sigma^2} = \sum_{i=1}^n\left(\frac{X_i - \overline{X}}{\sigma}\right)^2 \sim χ^2(n - 1)
$$

---

$\overline{X}$ 与 $S^2$ 相互独立:

$$
\frac{\sqrt{n}(\overline{X} - \mu)}{S} \sim t(n-1)
$$

$$
\frac{n(\overline{X} - \mu)^2}{S^2} \sim F(1, n-1)
$$

---

设 $X_1, X_2, \cdots, X_m$ 和 $Y_1, Y_2, \cdots, Y_n$ 来自两个正态总体 $X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$ 且相互独立, $\overline{X}, \overline{Y}, S_X^2, S_Y^2$ 相互独立。

$$
\overline{X} - \overline{Y} \sim 
N(\mu_1 - \mu_2, \frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n})
$$

$$
\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}
{\sqrt{\frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}}} \sim N(0, 1)
$$

$$
\frac{\sum\limits_{i=1}^m (X_i - \mu_1)^2/m\sigma_1^2}
{\sum\limits_{i=1}^n (Y_i - \mu_2)^2/n\sigma_2^2} 
\sim F(m, n)
$$

$$
\frac{S_X^2/\sigma_1^2}{S_Y^2/\sigma_2^2} = 
\frac{\sum\limits_{i=1}^m (X_i - \overline{X})^2/(m-1)\sigma_1^2}
{\sum\limits_{i=1}^n (Y_i - \overline{Y})^2/(n-1)\sigma_2^2}
\sim F(m-1, n-1)
$$

---

## 参数估计

### 矩估计

令样本矩 = 总体矩，写出 $\theta$ 表达式。

将样本均值，近似等于总体均值，进而利用样本均值来代替总体期望，然后利用该期望求得其他未知参数。

### 最大似然估计

写出似然函数：

$$
L(\theta) = L(x_1, x_2, \cdots, x_n; \theta) = 
\prod_{i=1}^n f(x_i;\theta)
$$

可能的话，对 $\theta$ 求导，导数赋值为0；

求处L函数最大值时，$\theta$的代数式。

---

**正态分布** 关于 $\mu$ 和 $\sigma^2$ 的 **矩估计** 和 **极大似然估计** 相等 $\displaystyle\hat{\mu} = \overline{X}$， $\displaystyle\hat{\sigma}^2 = \frac{\sum\limits_{i=1}^{n}(X_i - \overline{X})^2}{n}$

**指数分布** 关于 $\lambda$ 的 **矩估计** 和 **极大似然估计** 相等 $\hat{\lambda} = 1/\overline{X}$

**二项分布** 关于 $p$ 的 **矩估计** 和 **极大似然估计** 相等 $\hat{p} = \overline{X}/n$

**泊松分布** 关于 $\lambda$ 的 **矩估计** 和 **极大似然估计** 相等 $\hat{\lambda} = \overline{X}$

---

### 估计量的评价标准

- **无偏性**

若参数 $\theta$ 的估计量 $\hat\theta = \hat{\theta}(X_1, X_2, \cdots, X_n)$ 对一切 $n$ 及 $\theta \in \Theta$ 有 $E(\hat{\theta}) = \theta$，则称 $\hat{\theta}$ 为 $\theta$ 的 **无偏估计量**，否则为 **有偏估计量**。

- **有效性**

设 $\hat{\theta_1} = \hat{\theta_1}(X_1, X_2, \cdots, X_n)$ 与 $\hat{\theta_2} = \hat{\theta_2}(X_1, X_2, \cdots, X_n)$ 都是 $\theta$ 的无偏估计量，如果 $D(\hat{\theta_1}) < D(\hat{\theta_2})$，则称 $\hat{\theta_1}$ 比 $\hat{\theta_2}$ 有效。

- **一致性**

设 $\hat\theta = \hat{\theta}(X_1, X_2, \cdots, X_n)$ 为未知参数 $\theta$ 的估计量，如果对任意 $\varepsilon > 0$ 有

$$
\lim_{n\to \infty} P\{|\hat{\theta} - \theta| < \varepsilon\} = 1
$$

即 $\hat{\theta} \xrightarrow{P} \theta \ (n \to \infty)$，则称 $\hat{\theta}$ 为 $\theta$ 的 一致估计量。

---

### 参数的区间估计

设 $\theta$ 是总体 $X$ 的一个未知参数，对于给定 $\alpha(0< \alpha < 1)$，如果由样本 $X_1, X_2, \cdots, X_n$ 确定的统计量 $\hat{\theta_1} = \hat{\theta_1}(X_1, X_2, \cdots, X_n)$, $\hat{\theta_2} = \hat{\theta_2}(X_1, X_2, \cdots, X_n)$ 使

$$
P\{\hat{\theta_1}(X_1, X_2, \cdots, X_n) < \theta < \hat{\theta_2}(X_1, X_2, \cdots, X_n)\} = 1 - \alpha
$$

则称随机区间 $(\hat{\theta_1}, \hat{\theta_2})$ 是 $\theta$ 置信度为 $1 - \alpha$ 的 **置信区间**
 
$\hat{\theta_1}$ 和 $\hat{\theta_2}$ 分别称为 $\theta$ 的置信度为 $1 - \alpha$ 的双侧置信区间的 **置信下限** 和 **置信上线**

$1- \alpha$ 称为 **置信度** 或 **置信水平**，$α$ 称为 **显著性水平** 或 **误判风险**。

---

## 假设检验

- $H_0$ 原假设，包含等于，一般保护原假设，需要有充分的证据才能拒绝原假设。
- $H_1$ 备择假设
- 第一类错误：原假设为真时，拒绝（本来应该不拒绝）
- 第二类错误：原假设为假时，不拒绝（本来应该拒绝）

第一类错误和第二类错误是此消彼长的关系；另外这里用 **不拒绝** 比 **接受** 更加准确，接受是一种不得已的接受。

原假设是在一次试验中有 **绝对优势** 出现的事件，而备择假设在一次试验中不易发生(或几乎不可能发生)的事件。因此，在进行单侧检验时，最好把原假设取为预想结果的反面，即把希望证明的命题放在备择假设上。

将可能犯的严重错误看作第一类错误，因为犯第一类错误的概率可以通过 $\alpha$ 的大小来控制。犯第二类错误的概率是无法控制的。

如审判犯人时，可能会犯 **有罪判成无罪** 或者 **无罪判成有罪** 的错误，相比较而言，**无罪判成有罪** 的错误更严重，因为一般需要有充分的证据才能判一个人有罪。


----

**$\chi^2$ 检验**

$\displaystyle\frac{(n -1)S^2}{\sigma^2} \sim \chi^2(n - 1)$ 可以在不知道总体均值 $\mu$ 的情况下，来假设总体方差 $\sigma^2$

---

**T 检验**

$\displaystyle\frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t(n-1)$ 可以在不知道总体方差 $\sigma^2$ 的情况下，来假设总体的均值 $\mu$

---

**F 检验**

$\displaystyle\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2} \sim F(n_1 - 1, n_2 - 1)$

可以在不知道两个总体的均值，但知道其中某个方差的情况下，假设另一个方差。
